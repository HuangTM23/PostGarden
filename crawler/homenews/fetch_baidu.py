import argparse
import json
import re
import time
import os
import sys
from typing import Dict, List, Tuple

import requests
from bs4 import BeautifulSoup

# Selenium å¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# Constants
BOARD_API_URL = "https://top.baidu.com/api/board?platform=pc&tab=realtime"
USER_AGENT = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"

def get_headers() -> Dict[str, str]:
    return {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://top.baidu.com/board?tab=realtime",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

def get_no_proxy_session():
    """åˆ›å»ºä¸€ä¸ªä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†çš„ Session"""
    session = requests.Session()
    session.trust_env = False
    return session

def install_selenium_hint():
    """æç¤ºå®‰è£… Selenium"""
    print("\n" + "!"*50)
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Selenium åº“")
    print("!"*50)
    print("\nç™¾åº¦çƒ­æœéœ€è¦ Selenium æ¥ç»•è¿‡éªŒè¯å¹¶æå–çœŸå®æ–°é—»æºã€‚")
    print("\nğŸ“¦ å®‰è£…æ­¥éª¤ï¼š")
    print("\n1. å®‰è£… Seleniumï¼š")
    print("   pip install selenium webdriver-manager")
    print("\n2. ç¡®ä¿å·²å®‰è£… Chrome æµè§ˆå™¨")
    print("\nå®‰è£…å®Œæˆåï¼Œè¯·é‡æ–°è¿è¡Œæœ¬è„šæœ¬ã€‚")
    print("\n" + "!"*50 + "\n")
    sys.exit(1)

def init_driver():
    """åˆå§‹åŒ– Selenium WebDriver"""
    if not SELENIUM_AVAILABLE:
        install_selenium_hint()
    
    print("    [*] æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
    try:
        options = ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-proxy-server")
        options.add_argument("--disable-images")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(f"user-agent={USER_AGENT}")
        options.page_load_strategy = 'eager'
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # éšè— Selenium ç‰¹å¾
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
          "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
          """
        })
        
        driver.set_page_load_timeout(60)
        print("    [âœ“] æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver
    except Exception as e:
        print(f"    [!] æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}")
        return None

def fetch_top_list(limit: int = 9) -> List[Dict]:
    """ä»ç™¾åº¦çƒ­æœAPIè·å–æ¦œå•"""
    print(f"    [*] ä»APIè·å–å‰{limit}æ¡çƒ­æœ...")
    try:
        session = get_no_proxy_session()
        resp = session.get(BOARD_API_URL, headers=get_headers(), timeout=10)
        resp.raise_for_status()
        data = resp.json()
        
        cards = data.get("data", {}).get("cards", [])
        if not cards:
            print("    [!] API å“åº”ä¸­æœªæ‰¾åˆ°cards")
            return []
            
        content = cards[0].get("content", [])
        items = []
        for idx, item in enumerate(content[:limit], 1):
            search_url = item.get("url") or item.get("rawUrl") or ""
            items.append({
                "rank": idx,
                "title": item.get("word") or item.get("query") or "",
                "desc": item.get("desc") or "",
                "search_url": search_url,
                "image_url": item.get("img") or "",
                "hot_score": item.get("hotScore") or 0
            })
        
        print(f"    [âœ“] API è·å–æˆåŠŸï¼Œå…±{len(items)}æ¡")
        return items
    except Exception as e:
        print(f"    [!] API è·å–å¤±è´¥: {type(e).__name__}")
        return []

def extract_from_html(html: str) -> Tuple[str, str]:
    """ä»HTMLä¸­æå–çœŸå®URLå’Œæ¥æº"""
    found_url = ""
    found_source = ""

    # ç­–ç•¥1: ä»<!--s-data:{...}-->æå–
    s_data_pattern = r'<!--s-data:(.*?)-->'
    matches = re.findall(s_data_pattern, html, re.DOTALL)
    
    if matches:
        for match in matches:
            try:
                data = json.loads(match)
                
                # å­ç­–ç•¥1: citationList (ä¼˜å…ˆ)
                card_data = data.get("cardData", {})
                citation_list = card_data.get("citationList", {})
                
                if citation_list:
                    ref_data = citation_list.get("data", {})
                    ref_list = ref_data.get("referenceList", [])
                    
                    if ref_list and isinstance(ref_list, list) and len(ref_list) > 0:
                        first_ref = ref_list[0]
                        real_url = first_ref.get("url", "")
                        source = first_ref.get("source", "")
                        
                        if isinstance(source, dict):
                            source = source.get("name", "")
                        
                        if real_url:
                            found_url = real_url
                            found_source = str(source)
                            if found_source:
                                break

                # å­ç­–ç•¥2: blocksList (å¤‡é€‰)
                if not found_url:
                    blocks_list = data.get("cardData", {}).get("blocksList", [])
                    for block in blocks_list:
                        items = block.get("data", {}).get("items", [])
                        for item in items:
                            source_list = item.get("sourceList", [])
                            if source_list and isinstance(source_list, list) and len(source_list) > 0:
                                src_text = source_list[0].get("text", "")
                                if src_text:
                                    link_info = item.get("linkInfo", {})
                                    link = link_info.get("href", "") or link_info.get("url", "")
                                    
                                    found_url = link
                                    found_source = src_text
                                    if found_url and found_source:
                                        break
                        if found_url and found_source:
                            break

            except (json.JSONDecodeError, Exception):
                continue
            
            if found_url and found_source:
                break
    else:
        # å¤‡é€‰: ç®€å•çš„ç™¾å®¶å·é“¾æ¥
        bjh_pattern = r'https://baijiahao\.baidu\.com/s\?id=\d+'
        bjh_matches = re.findall(bjh_pattern, html)
        if bjh_matches:
            found_url = bjh_matches[0]
            found_source = "Baidu (Fallback)"

    # ç­–ç•¥2: BeautifulSoup DOM è§£æ
    if not found_source or found_source == "Baidu (Fallback)":
        try:
            soup = BeautifulSoup(html, "html.parser")
            
            # æŸ¥æ‰¾ cosc-source-text
            cosc_source = soup.select_one(".cosc-source-text")
            if cosc_source:
                src_text = cosc_source.get_text(strip=True)
                if src_text:
                    found_source = src_text
                    
                    if not found_url:
                        link_el = soup.select_one("a.title_dIF3B, a.c-blocka")
                        found_url = link_el.get("href", "") if link_el else ""

            # æ›´å¤šæ ‡å‡†é€‰æ‹©å™¨
            if not found_source:
                source_el = soup.select_one(".c-showurl, .source_1V_v6, .c-source, .c-gray, .c-color-gray")
                if source_el:
                    src_text = source_el.get_text(strip=True)
                    src_text = re.sub(r'\d{4}-\d{2}-\d{2}.*', '', src_text).strip()
                    src_text = src_text.split(' ')[0]
                    if src_text and len(src_text) < 20:
                        found_source = src_text
                
                if not found_source:
                    first_result = soup.select_one(".result, .c-container, .new-pmd")
                    if first_result:
                        source_el = first_result.select_one(".source-text, .c-color-gray, span.c-gray, .newTimeFactor_vocab, .c-source")
                        if source_el:
                            src_text = source_el.get_text(strip=True)
                            if src_text and len(src_text) < 20 and ":" not in src_text:
                                found_source = src_text
                            
                    if not found_url:
                        link_el = first_result.select_one("a") if first_result else None
                        found_url = link_el.get("href", "") if link_el else ""

        except Exception:
            pass

    return found_url, found_source

def resolve_real_source(search_url: str, driver=None) -> Tuple[str, str]:
    """è®¿é—®æœç´¢é¡µé¢æå–çœŸå®URLå’Œæ¥æºï¼ˆSeleniumä¼˜å…ˆï¼‰"""
    if not search_url:
        return "", "ç™¾åº¦"

    print(f"        æ­£åœ¨è§£æ: {search_url[:60]}...")
    html = ""
    
    # ä½¿ç”¨ Selenium
    if driver:
        try:
            driver.get(search_url)
            time.sleep(2)
            html = driver.page_source
            
            # æ£€æµ‹éªŒè¯ç 
            if "ç™¾åº¦å®‰å…¨éªŒè¯" in driver.title or "security-verification" in html:
                print(f"        [!] æ£€æµ‹åˆ°ç™¾åº¦å®‰å…¨éªŒè¯")
                return search_url, "ç™¾åº¦"
                
        except Exception as e:
            print(f"        [!] Selenium é”™è¯¯: {type(e).__name__}")
            return search_url, "ç™¾åº¦"
    else:
        # å¤‡é€‰: requests
        try:
            session = get_no_proxy_session()
            resp = session.get(search_url, headers=get_headers(), timeout=10)
            if "wappass.baidu.com" in resp.url or "security-verification" in resp.text:
                print(f"        [!] æ£€æµ‹åˆ°éªŒè¯ç  (requests)")
                return search_url, "ç™¾åº¦"
            html = resp.text
        except Exception as e:
            print(f"        [!] Request é”™è¯¯: {type(e).__name__}")
            return search_url, "ç™¾åº¦"

    # è§£æ HTML
    real_url, source = extract_from_html(html)
    
    # å¦‚æœæ˜¯ç™¾å®¶å·ä¸”æ¥æºæœªçŸ¥,å°è¯•è¿›ä¸€æ­¥è§£æ
    if real_url and "baijiahao.baidu.com" in real_url and source == "Baidu (Fallback)":
        print(f"        [+] è§£æç™¾å®¶å·æ¥æº: {real_url[:50]}...")
        bj_source = resolve_baijiahao_source(real_url, driver=driver)
        if bj_source:
            source = bj_source
    
    final_url = real_url if real_url else search_url
    final_source = source if source else "ç™¾åº¦"
    
    return final_url, final_source

def resolve_baijiahao_source(url: str, driver=None) -> str:
    """è®¿é—®ç™¾å®¶å·é¡µé¢æå–ä½œè€…å"""
    try:
        html = ""
        if driver:  # Selenium driver object
            driver.get(url)
            time.sleep(2)
            html = driver.page_source
        else:
            session = get_no_proxy_session()
            resp = session.get(url, headers=get_headers(), timeout=10)
            html = resp.text
            
        soup = BeautifulSoup(html, "html.parser")
        
        # 1. ä½œè€…åé€‰æ‹©å™¨
        author = soup.select_one(".author-name, span.author-name, a.author-name, span[class*='author'], a[class*='author']")
        if author:
            name = author.get_text(strip=True)
            if 1 < len(name) < 30:
                return name

        # 2. Meta æ ‡ç­¾
        meta = soup.find("meta", attrs={"property": "og:site_name"})
        if meta and meta.get("content"):
            return meta["content"].strip()
            
        meta = soup.find("meta", attrs={"name": "source"})
        if meta and meta.get("content"):
            return meta["content"].strip()

    except Exception as e:
        print(f"        [!] è§£æç™¾å®¶å·å¤±è´¥: {type(e).__name__}")
        
    return ""

def get_baidu_news(count: int = 9) -> List[dict]:
    """
    æŠ“å–ç™¾åº¦çƒ­æœæ–°é—»
    :param count: è¿”å›æ•°é‡
    :return: JSONæ ¼å¼çš„åˆ—è¡¨
    """
    print("[Baidu] å¼€å§‹æŠ“å–çƒ­æœæ–°é—»...")
    
    # æ£€æŸ¥ Selenium
    if not SELENIUM_AVAILABLE:
        install_selenium_hint()
    
    # 1. è·å–çƒ­æ¦œåˆ—è¡¨
    items = fetch_top_list(limit=count)
    if not items:
        print("[Baidu] âœ— æœªè·å–åˆ°ä»»ä½•æ–°é—»")
        return []
    
    print(f"[Baidu] âœ“ è·å–{len(items)}æ¡å€™é€‰æ–°é—»")
    results = []
    
    # 2. åˆå§‹åŒ– Selenium
    driver = init_driver()
    if not driver:
        print("[Baidu] âœ— æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
        return []
    
    try:
        for item in items:
            print(f"\n[Baidu] å¤„ç†ç¬¬{item['rank']}/{len(items)}æ¡:")
            print(f"  æ ‡é¢˜: {item['title']}")
            
            # è§£æçœŸå®æ¥æº
            real_url, source_name = resolve_real_source(item['search_url'], driver=driver)
            
            print(f"  æ¥æº: {source_name}")
            
            # å†…å®¹ä¼˜å…ˆä½¿ç”¨desc
            content_val = item['desc'] or item['title']
            if len(content_val) > 100:
                content_preview = content_val[:100] + "..."
            else:
                content_preview = content_val
            print(f"  å†…å®¹: {content_preview}")
            print(f"  é“¾æ¥: {real_url[:60]}...")
            
            results.append({
                "rank": len(results) + 1,
                "title": item['title'],
                "title0": "",
                "content": content_val,
                "index": item['hot_score'],
                "author": "baidu",
                "source_platform": source_name,
                "source_url": real_url,
                "image": item['image_url']
            })
            print(f"  âœ“ ç¬¬{len(results)}æ¡æ–°é—»å·²ä¿å­˜")
            
            time.sleep(0.5)
    
    finally:
        if driver:
            driver.quit()
            print("    [âœ“] æµè§ˆå™¨å·²å…³é—­")
    
    print(f"\n[Baidu] âœ“ æŠ“å–å®Œæˆï¼Œå…±{len(results)}æ¡æ–°é—»\n")
    return results

def main(limit: int = 9):
    results = get_baidu_news(count=limit)
    if results:
        print(json.dumps(results, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Baidu Hot News Scraper")
    parser.add_argument("--limit", type=int, default=9, help="Number of items to scrape")
    args = parser.parse_args()
    
    main(limit=args.limit)

