import json
import time
import random
import requests
import sys
import re
import os
from typing import Tuple, List
from bs4 import BeautifulSoup

def get_no_proxy_session():
    """åˆ›å»ºä¸€ä¸ªä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†çš„ Session"""
    session = requests.Session()
    session.trust_env = False
    return session

# Selenium å¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://news.qq.com/",
}

# è…¾è®¯æ–°é—»æ ‡ç­¾é¡µIDï¼ˆæ—©æŠ¥çƒ­ç‚¹ï¼‰
TAG_ID = "aEWqxLtdgmQ="

def install_selenium_hint():
    """æç¤ºå®‰è£… Selenium"""
    print("\n" + "!"*50)
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Selenium åº“")
    print("!"*50)
    print("\nSelenium æ˜¯æµè§ˆå™¨è‡ªåŠ¨åŒ–åº“,è…¾è®¯æ–°é—»éœ€è¦å®ƒæ¥è·å–åŠ¨æ€å†…å®¹ã€‚")
    print("\nğŸ“¦ å®‰è£…æ­¥éª¤ï¼š")
    print("\n1. å®‰è£… Selenium ç›¸å…³åº“ï¼š")
    print("   pip install selenium webdriver-manager")
    print("\n2. ç¡®ä¿å·²å®‰è£… Chrome æµè§ˆå™¨")
    print("\nå®‰è£…å®Œæˆåï¼Œè¯·é‡æ–°è¿è¡Œæœ¬è„šæœ¬ã€‚")
    print("\n" + "!"*50 + "\n")
    sys.exit(1)

def init_driver():
    """åˆå§‹åŒ– Selenium WebDriver"""
    if not SELENIUM_AVAILABLE:
        install_selenium_hint()
    
    print("    [*] æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
    try:
        options = ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-proxy-server")
        options.add_argument("--disable-images")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(f"user-agent={HEADERS['User-Agent']}")
        options.page_load_strategy = 'eager'
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # éšè— Selenium ç‰¹å¾
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
          "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
          """
        })
        
        driver.set_page_load_timeout(60)
        print("    [âœ“] æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver
    except Exception as e:
        print(f"    [!] æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}")
        return None

def get_links_with_selenium(tag_id: str, count: int, driver) -> List[str]:
    """ä½¿ç”¨ Selenium è·å–åŠ¨æ€åŠ è½½çš„é“¾æ¥"""
    url = f"https://news.qq.com/tag/{tag_id}"
    print(f"    [*] ä½¿ç”¨ Selenium è®¿é—®ï¼š{url}")
    print(f"    [*] ç›®æ ‡ï¼šæŠ“å–å‰ {count} æ¡é“¾æ¥...")

    links = []
    
    try:
        driver.get(url)
        time.sleep(3)  # ç­‰å¾…åˆå§‹åŠ è½½
        
        print(f"    [*] é¡µé¢æ ‡é¢˜: {driver.title}")
        
        # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
        last_height = driver.execute_script("return document.body.scrollHeight")
        retry_count = 0
        
        while len(links) < count and retry_count < 3:
            # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            from selenium.webdriver.common.by import By
            all_links = driver.find_elements(By.TAG_NAME, 'a')
            
            for link_element in all_links:
                try:
                    href = link_element.get_attribute('href')
                    
                    if not href:
                        continue
                    
                    # ç­›é€‰æœ‰æ•ˆçš„è…¾è®¯æ–°é—»é“¾æ¥
                    if ('/rain/a/' in href or '/omn/' in href) and 'news.qq.com' in href:
                        # æ’é™¤æ— æ•ˆé“¾æ¥
                        if not any(x in href for x in ['author', 'video', 'zt', 'live']):
                            # æ¸…ç† URL
                            if '#' in href:
                                href = href.split('#')[0]
                            
                            if href not in links:
                                links.append(href)
                                if len(links) >= count:
                                    break
                except:
                    continue
            
            if len(links) >= count:
                break
            
            # æ»šåŠ¨åˆ°åº•éƒ¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                retry_count += 1
            else:
                retry_count = 0
            last_height = new_height
        
        print(f"    [âœ“] Selenium è·å–æˆåŠŸï¼Œæ‰¾åˆ° {len(links)} æ¡é“¾æ¥")
        
    except Exception as e:
        print(f"    [!] Selenium å‡ºé”™: {type(e).__name__}")
    
    return links[:count]

def get_article_details(url: str, max_retries: int = 2) -> Tuple[str, str, str, str]:
    """è·å–æ–‡ç« è¯¦æƒ…ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                time.sleep(2)
            
            session = get_no_proxy_session()
            response = session.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹æ˜¯å¦ä¸ºè§†é¢‘ URL
            is_video = bool(re.search(r'/[a-zA-Z0-9]*V[a-zA-Z0-9]*', url))
            
            # --- 1. æå–æ ‡é¢˜ ---
            title = ""
            title_tag = soup.find('h1') or \
                       soup.find('div', class_='video-title') or \
                       soup.find('h2', class_='title')
            
            if title_tag:
                title = title_tag.get_text(strip=True)
            
            if not title or is_video:
                if soup.title:
                    full_title = soup.title.get_text().strip()
                    title = full_title.split('_')[0].strip()
            
            if not title:
                title = "æœªæ‰¾åˆ°æ ‡é¢˜"
            
            # --- 2. æå–å†…å®¹ ---
            content = ""
            content_div = soup.find('div', class_='content-article') or soup.find('div', id='ArticleContent')
            
            if content_div:
                content = content_div.get_text(strip=True)[:200]
            else:
                if is_video:
                    desc = soup.find('div', class_='video-desc') or \
                           soup.find('p', class_='desc') or \
                           soup.find('meta', attrs={'name': 'description'})
                    
                    if desc:
                        if hasattr(desc, 'get_text'):
                            content = desc.get_text(strip=True)
                        elif 'content' in desc.attrs:
                            content = desc['content']
                    
                    if not content or len(content) < 5:
                        content = title
                else:
                    paragraphs = soup.find_all('p')
                    valid_ps = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10]
                    content = "\n".join(valid_ps[:3]) if valid_ps else title
            
            if not content:
                content = title
            
            # --- 3. æå–æ¥æº ---
            source_platform = "è…¾è®¯æ–°é—»"
            
            author_info = soup.select_one(".author-info .name, .media-info .media-name, .author-name, .media-name")
            if author_info:
                source_platform = author_info.get_text(strip=True)
            
            if source_platform == "è…¾è®¯æ–°é—»":
                author_meta = soup.find('meta', property='article:author') or \
                             soup.find('meta', attrs={'name': 'author'})
                if author_meta:
                    source_platform = author_meta.get('content', source_platform)
            
            if source_platform == "è…¾è®¯æ–°é—»":
                media_elem = soup.find('div', class_='author-txt') or \
                            soup.find('div', class_='author-name') or \
                            soup.find('span', class_='media-name')
                if media_elem:
                    source_platform = media_elem.get_text(strip=True)
            
            # --- 4. æå–å›¾ç‰‡ ---
            cover_image = ""
            
            og_img = soup.find('meta', property='og:image')
            if og_img:
                cover_image = og_img.get('content', '')
            
            if not cover_image or "default" in cover_image or "logo" in cover_image:
                if content_div:
                    img = content_div.find('img')
                    if img:
                        cover_image = img.get('data-src') or img.get('src') or cover_image
            
            if not cover_image and is_video:
                video_tag = soup.find('video')
                if video_tag:
                    cover_image = video_tag.get('poster', '')
            
            if cover_image:
                if cover_image.startswith('//'):
                    cover_image = 'https:' + cover_image
                if "logo_gray" in cover_image or "default" in cover_image:
                    cover_image = ""
            
            return title, content, source_platform, cover_image
            
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                continue
        except Exception as e:
            if attempt < max_retries - 1:
                continue
    
    return "", "", "è…¾è®¯æ–°é—»", ""

def get_tencent_news(count: int = 9) -> List[dict]:
    """
    æŠ“å–è…¾è®¯æ—©æŠ¥æ–°é—»ï¼ˆä½¿ç”¨ Seleniumï¼‰
    :param count: è¿”å›æ•°é‡
    :return: JSONæ ¼å¼çš„åˆ—è¡¨
    """
    print("[Tencent] å¼€å§‹æŠ“å–æ—©æŠ¥æ–°é—»...")
    
    # åˆå§‹åŒ–æµè§ˆå™¨
    driver = init_driver()
    if not driver:
        print("[Tencent] âœ— æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
        return []
    
    try:
        # ä½¿ç”¨ Selenium è·å–é“¾æ¥
        links = get_links_with_selenium(TAG_ID, count, driver)
        
        if not links:
            print("[Tencent] âœ— æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"[Tencent] âœ“ è·å–{len(links)}æ¡æ–‡ç« é“¾æ¥ï¼Œå¼€å§‹è§£æå†…å®¹...")
        results = []
        
        for idx, link in enumerate(links, 1):
            if len(results) >= count:
                break
            
            print(f"\n[Tencent] å¤„ç†ç¬¬{len(results)+1}/{count}æ¡:")
            print(f"  é“¾æ¥: {link}")
            
            title, content, source_platform, cover_image = get_article_details(link)
            
            if not title:
                print(f"  âœ— æ ‡é¢˜è·å–å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¡")
                continue
            
            print(f"  æ ‡é¢˜: {title}")
            print(f"  æ¥æº: {source_platform}")
            
            if len(content) > 100:
                content_preview = content[:100] + "..."
            else:
                content_preview = content
            print(f"  å†…å®¹: {content_preview}")
            
            if cover_image:
                print(f"  å›¾ç‰‡: {cover_image[:50]}...")
            
            results.append({
                "rank": len(results) + 1,
                "title": title,
                "title0": "",
                "content": content,
                "index": 0,
                "author": "tencent",
                "source_platform": source_platform,
                "source_url": link,
                "image": cover_image
            })
            print(f"  âœ“ ç¬¬{len(results)}æ¡æ–°é—»å·²ä¿å­˜")
            
            time.sleep(random.uniform(0.5, 1.0))
        
        print(f"\n[Tencent] âœ“ æŠ“å–å®Œæˆï¼Œå…±{len(results)}æ¡æ–°é—»\n")
        return results
    
    finally:
        if driver:
            driver.quit()
            print("    [âœ“] æµè§ˆå™¨å·²å…³é—­")

if __name__ == "__main__":
    import sys
    count = int(sys.argv[1]) if len(sys.argv) > 1 else 9
    result = get_tencent_news(count)
    print(json.dumps(result, ensure_ascii=False, indent=2))
