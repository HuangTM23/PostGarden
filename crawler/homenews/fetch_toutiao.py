# Renamed from fetch_toutiao_hot.py
import argparse
import json
import time
import random
import requests
import os
import re
import sys
from typing import Tuple, List
from bs4 import BeautifulSoup

# Selenium å¯¼å…¥
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.common.by import By
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

API_URL = "https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

HEADERS = {
    "User-Agent": USER_AGENT,
    "Accept": "*/*",
    "Referer": "https://www.toutiao.com/"
}

def get_no_proxy_session():
    """åˆ›å»ºä¸€ä¸ªä¸ä½¿ç”¨ç³»ç»Ÿä»£ç†çš„ Session"""
    session = requests.Session()
    session.trust_env = False
    return session

def install_selenium_hint():
    """æç¤ºå®‰è£… Selenium"""
    print("\n" + "!"*50)
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Selenium åº“")
    print("!"*50)
    print("\nSelenium æ˜¯æµè§ˆå™¨è‡ªåŠ¨åŒ–åº“,ä»Šæ—¥å¤´æ¡éœ€è¦å®ƒæ¥æå–çœŸå®æ–°é—»æ¥æºã€‚")
    print("\nğŸ“¦ å®‰è£…æ­¥éª¤ï¼š")
    print("\n1. å®‰è£… Selenium ç›¸å…³åº“ï¼š")
    print("   pip install selenium webdriver-manager")
    print("\n2. å®‰è£… Chrome æµè§ˆå™¨ï¼ˆå¦‚æœªå®‰è£…ï¼‰")
    print("\nå®‰è£…å®Œæˆåï¼Œè¯·é‡æ–°è¿è¡Œæœ¬è„šæœ¬ã€‚")
    print("\n" + "!"*50 + "\n")
    sys.exit(1)

def init_driver():
    """åˆå§‹åŒ– Selenium WebDriver"""
    if not SELENIUM_AVAILABLE:
        install_selenium_hint()
    
    print("    [*] æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
    try:
        options = ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-proxy-server")
        options.add_argument("--disable-images")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(f"user-agent={USER_AGENT}")
        options.page_load_strategy = 'eager'
        
        service = ChromeService(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        # éšè— Selenium ç‰¹å¾
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
          "source": """
            Object.defineProperty(navigator, 'webdriver', {
              get: () => undefined
            })
          """
        })
        
        driver.set_page_load_timeout(60)
        print("    [âœ“] æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver
    except Exception as e:
        print(f"    [!] æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}")
        return None

def fetch_hot_list(limit: int = 9) -> List[dict]:
    """ä»ä»Šæ—¥å¤´æ¡çƒ­æ¦œAPIè·å–é“¾æ¥"""
    try:
        print("    [*] ä»çƒ­æ¦œAPIè·å–æ–‡ç« åˆ—è¡¨...")
        session = get_no_proxy_session()
        response = session.get(API_URL, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        items = data.get('data', [])[:limit]
        
        if items:
            print(f"    [âœ“] çƒ­æ¦œAPIè·å–æˆåŠŸï¼Œè·å¾— {len(items)} æ¡é“¾æ¥")
        return items
        
    except Exception as e:
        print(f"    [!] çƒ­æ¦œAPIè·å–å¤±è´¥: {type(e).__name__}")
        return []

def resolve_article_data(rank: int, title: str, initial_url: str, driver) -> Tuple[str, str, str, str]:
    """
    æ ¸å¿ƒé€»è¾‘ï¼š
    1. è®¿é—®åˆå§‹URL
    2. å¦‚æœæ˜¯trendingé¡µé¢,æ‰¾åˆ°å…·ä½“å†…å®¹é“¾æ¥
    3. è®¿é—®å†…å®¹é¡µé¢
    4. æå–çœŸå®æ¥æºå’Œå†…å®¹
    """
    source_platform = "ä»Šæ—¥å¤´æ¡"
    source_url = initial_url
    content = title
    image_url = ""
    
    try:
        # æ­¥éª¤1 & 2: è§£æç›®æ ‡URL
        if "/trending/" in initial_url:
            driver.get(initial_url)
            target_href = None
            link_type = "unknown"
            
            try:
                # ç­‰å¾…å†…å®¹é“¾æ¥å‡ºç°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, 
                        "//a[contains(@href, '/video/') or contains(@href, '/w/') or contains(@href, '/article/')]"))
                )
                links = driver.find_elements(By.XPATH, 
                    "//a[contains(@href, '/video/') or contains(@href, '/w/') or contains(@href, '/article/')]")
                
                for link in links:
                    h = link.get_attribute("href")
                    if not h:
                        continue
                    
                    if "/video/" in h and re.search(r"/video/\d+", h):
                        target_href = h
                        link_type = "video"
                        break
                    if "/w/" in h and re.search(r"/w/\d+", h):
                        target_href = h
                        link_type = "w"
                        break
                    if "/article/" in h and re.search(r"/article/\d+", h):
                        target_href = h
                        link_type = "article"
                        break
            except Exception as e:
                print(f"        æœªæ‰¾åˆ°å†…å®¹é“¾æ¥: {type(e).__name__}")
            
            if target_href:
                source_url = target_href
                if source_url.startswith("//"):
                    source_url = "https:" + source_url
                elif source_url.startswith("/"):
                    source_url = "https://www.toutiao.com" + source_url
        else:
            link_type = "article"
        
        # æ­¥éª¤3: è®¿é—®ç›®æ ‡å†…å®¹é¡µé¢
        if source_url != driver.current_url:
            driver.get(source_url)
        
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # æ­¥éª¤4: æå–æ•°æ®
        
        # A. æå–æ¥æºå¹³å°
        platform_el = soup.select_one(".author-info .name, .article-meta .name, .author-name, .user-card-name, .media-info .name")
        
        if platform_el:
            source_platform = platform_el.get_text(strip=True)
        else:
            meta_name = soup.find('meta', attrs={'name': 'author'}) or \
                       soup.find('meta', property='og:site_name')
            if meta_name:
                source_platform = meta_name.get('content', 'ä»Šæ—¥å¤´æ¡')
        
        # B. æå–å†…å®¹
        if link_type == "video" or "/video/" in source_url:
            # è§†é¢‘ï¼šä½¿ç”¨æ ‡é¢˜
            content = title
        else:
            # æ–‡ç« /å¾®å¤´æ¡
            article_tag = soup.select_one('article.syl-page-article, article.tt-article-content, article.syl-article-base')
            
            if article_tag:
                content = article_tag.get_text(separator="\n", strip=True)[:200]
            else:
                w_div = soup.select_one(".weitoutiao-html")
                if w_div:
                    content = w_div.get_text(separator="\n", strip=True)[:200]
                else:
                    ps = soup.select(".article-content p, article p")
                    if ps:
                        content = "\n".join([p.get_text(strip=True) for p in ps[:3]])
        
        if not content:
            content = title
        
        # C. æå–å›¾ç‰‡
        og_img = soup.find('meta', property='og:image')
        if og_img:
            image_url = og_img.get('content', '')
            if image_url.startswith('//'):
                image_url = 'https:' + image_url
        
        print(f"        æ¥æº: {source_platform}")
        
    except Exception as e:
        print(f"        [!] å¤„ç†å¤±è´¥: {type(e).__name__}")
    
    return source_platform, source_url, content, image_url

def get_toutiao_news(count: int = 9) -> List[dict]:
    """
    æŠ“å–ä»Šæ—¥å¤´æ¡æ–°é—»ï¼ˆæå–çœŸå®æ–°é—»æ¥æºï¼‰
    :param count: è¿”å›æ•°é‡
    :return: JSONæ ¼å¼çš„åˆ—è¡¨
    """
    print("[Toutiao] å¼€å§‹æŠ“å–çƒ­æœæ–°é—»...")
    
    # åˆå§‹åŒ–æµè§ˆå™¨
    driver = init_driver()
    if not driver:
        print("[Toutiao] âœ— æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
        return []
    
    try:
        # è·å–çƒ­æ¦œé“¾æ¥
        items = fetch_hot_list(limit=count)
        
        if not items:
            print("[Toutiao] âœ— æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            return []
        
        print(f"[Toutiao] âœ“ è·å–{len(items)}æ¡æ–‡ç« é“¾æ¥")
        results = []
        
        for idx, item in enumerate(items, 1):
            if len(results) >= count:
                break
            
            title = item.get("Title", "")
            initial_url = item.get("Url", "")
            hot_index = item.get("HotValue", 0)
            api_image = item.get("Image", {}).get("url", "") if isinstance(item.get("Image"), dict) else ""
            
            print(f"\n[Toutiao] å¤„ç†ç¬¬{len(results)+1}/{count}æ¡:")
            print(f"  æ ‡é¢˜: {title}")
            print(f"  çƒ­åº¦: {hot_index}")
            
            # è§£ææ–‡ç« æ•°æ®
            source_platform, source_url, content, image_url = resolve_article_data(
                len(results) + 1, title, initial_url, driver
            )
            
            if not source_platform:
                print(f"  âœ— è§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤æ¡")
                continue
            
            if len(content) > 100:
                content_preview = content[:100] + "..."
            else:
                content_preview = content
            print(f"  å†…å®¹: {content_preview}")
            
            # ä¼˜å…ˆä½¿ç”¨é¡µé¢æå–çš„å›¾ç‰‡,å¤‡é€‰APIå›¾ç‰‡
            final_image = image_url or api_image
            if final_image:
                print(f"  å›¾ç‰‡: {final_image[:50]}...")
            
            results.append({
                "rank": len(results) + 1,
                "title": title,
                "title0": "",
                "content": content,
                "index": hot_index,
                "author": "toutiao",
                "source_platform": source_platform,  # çœŸå®æ–°é—»æº
                "source_url": source_url,
                "image": final_image
            })
            print(f"  âœ“ ç¬¬{len(results)}æ¡æ–°é—»å·²ä¿å­˜")
            
            time.sleep(random.uniform(0.8, 1.5))
        
        print(f"\n[Toutiao] âœ“ æŠ“å–å®Œæˆï¼Œå…±{len(results)}æ¡æ–°é—»\n")
        return results
    
    finally:
        if driver:
            driver.quit()
            print("    [âœ“] æµè§ˆå™¨å·²å…³é—­")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Toutiao Hot News Scraper")
    parser.add_argument("--limit", type=int, default=9, help="Number of items to scrape")
    args = parser.parse_args()
    
    result = get_toutiao_news(count=args.limit)
    print(json.dumps(result, ensure_ascii=False, indent=2))