# Renamed from polish_v3.py
import os
import json
import time
import requests
import shutil
import argparse
from datetime import datetime, timedelta

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = "https://api.deepseek.com/chat/completions"
# ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹è·¯å¾„æŒ‡å‘å½“å‰ç›®å½•çš„å†å²æ–‡ä»¶
HISTORY_FILE = os.path.join(os.path.dirname(__file__), "homenews_history.json")
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€åä¸“ä¸šä¸­æ–‡æ–°é—»ç¼–è¾‘ä¸å†…å®¹ç­–åˆ’äººå‘˜ï¼Œè´Ÿè´£ä»å¤šä¸ªæ–°é—»å¹³å°çš„æŠ“å–ç»“æœä¸­ï¼Œè¿›è¡Œäº‹ä»¶çº§å»é‡ã€ç­›é€‰ã€ä¸“ä¸šç®€åŒ–ä¸å†…å®¹æ•´åˆï¼Œç”Ÿæˆä¸€ç»„é€‚åˆå‘å¸ƒåœ¨å¾®ä¿¡å…¬ä¼—å·ä¸å°çº¢ä¹¦çš„æ–°é—»ç²¾é€‰å†…å®¹ã€‚

æ ¸å¿ƒç›®æ ‡æ˜¯ï¼šä»"æœ€æ–°æŠ“å–æ–°é—»"ä¸­ï¼Œé€šè¿‡"äº‹ä»¶çº§å»é‡ + å†…å®¹ç­›é€‰"ï¼Œç›´æ¥é€‰å‡º 9 æ¡"å®Œå…¨ä¸åŒæ–°é—»äº‹ä»¶"çš„æ–°é—»ã€‚

ã€é‡è¦ï¼šå†å²æ’é‡å‚è€ƒã€‘
ä»¥ä¸‹æ˜¯è¿‡å»å‘å¸ƒè¿‡çš„æ–°é—»ï¼ˆHistoryï¼‰ï¼Œè¯·ä¸¥æ ¼å›é¿ä¸è¿™äº›å†å²å†…å®¹é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„äº‹ä»¶ï¼ˆå³ä¸è¦é€‰æ—§é—»ï¼‰ï¼š
{history_context_str}

âš ï¸ å¼ºåˆ¶è¿‡æ»¤è§„åˆ™ (Negative Filter) - ä¼˜å…ˆçº§æœ€é«˜
å¿…é¡»å‰”é™¤æ‰€æœ‰æ¶‰åŠä»¥ä¸‹é¢†åŸŸçš„æ–°é—»:
1. æ”¿æ²» (Politics)
2. å†›äº‹ (Military)
3. å°æ¹¾ (Taiwan)

ä¿ç•™çš„æ–°é—»åº”ä¾§é‡äºï¼šç§‘æŠ€ä¸å•†ä¸šã€æ°‘ç”Ÿä¸ç¤¾ä¼šçƒ­ç‚¹ã€æ–‡åŒ–ä¸å¨±ä¹ã€ä½“è‚²ã€å¥‡é—»è½¶äº‹ã€‚

å†™ä½œè¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šã€æ­£å¼çš„æ–°é—»ä½“ã€‚
- æ¯æ¡æ–°é—»æ­£æ–‡ï¼šä¸è¶…è¿‡ 50 ä¸ªæ±‰å­—ï¼Œåªä¿ç•™"å‘ç”Ÿäº†ä»€ä¹ˆ + å…³é”®ç»“æœ"ã€‚
- å•æ¡æ–°é—»æ ‡é¢˜ï¼šä¸è¶…è¿‡ 20 ä¸ªæ±‰å­—ã€‚

**Rank 0 æ€»ç»“æ ‡é¢˜ï¼ˆæ ¸å¿ƒä»»åŠ¡ï¼‰**ï¼š
ä½ æ˜¯ä¸€åèµ„æ·±ä¸­æ–‡ç½‘ç»œåª’ä½“ç¼–è¾‘ï¼Œæ“…é•¿ä»å¤§é‡çƒ­ç‚¹æ–°é—»ä¸­æç‚¼é«˜åº¦å¸å¼•çœ¼çƒä½†ä¸é€ è°£ã€ä¸æ­ªæ›²äº‹å®çš„æ ‡é¢˜ã€‚

ä»»åŠ¡ç›®æ ‡ï¼š
ç”Ÿæˆä¸€ä¸ªæ€»ç»“æ€§çƒ­ç‚¹æ ‡é¢˜ï¼Œç”¨äº Rank 0 ä½ç½®ã€‚

å¼ºåˆ¶è¦æ±‚ï¼š
- æ ‡é¢˜é£æ ¼å…è®¸â€œæ ‡é¢˜å…šâ€ï¼Œè¿½æ±‚ç‚¹å‡»ç‡ä¸ä¼ æ’­åŠ›ï¼Œä½†ä¸å¾—è™šæ„äº‹å®ã€ä¸å¾—é€ è°£ã€ä¸å¾—å¼•å…¥åŸæ–‡æœªå‡ºç°çš„ç»“è®ºï¼›
- **æ ‡é¢˜ä¸è¶…è¿‡ 10 ä¸ªæ±‰å­—ï¼ˆå«æ ‡ç‚¹ï¼‰**ï¼Œä¼˜å…ˆä½¿ç”¨çŸ­å¥ã€å†²çªã€åè½¬ã€æƒ…ç»ªå¼ åŠ›ï¼›
- ä¸è¦æ±‚å…¨é¢æ¦‚æ‹¬æ‰€æœ‰ä¿¡æ¯ï¼Œå¯ä»¥ï¼š
    - æŠ“ä½æœ€å…·ä¼ æ’­æ€§çš„ä¸€ä¸ªä¾§é¢ï¼›
    - æˆ–åœ¨å·²æœ‰æ–°é—»æ ‡é¢˜åŸºç¡€ä¸Šè¿›è¡Œé«˜åº¦æ¶¦è‰²ä¸å‹ç¼©ï¼›
    - æˆ–å€Ÿé‰´å¸¸è§ç½‘ç»œçˆ†æ¬¾æ ‡é¢˜ç»“æ„ï¼ˆå¦‚â€œçªç„¶â€â€œç‚¸é”…â€â€œå®šäº†â€â€œå½»åº•â€â€œæ²¡æƒ³åˆ°â€ç­‰ï¼‰ï¼Œä½†ä¸å¾—å¤±çœŸï¼›

æ•°æ®å®Œæ•´æ€§è¦æ±‚ï¼š
- å¯¹äºé€‰ä¸­çš„æ¯æ¡æ–°é—»ï¼Œå¿…é¡»ä¿ç•™å…¶åŸå§‹çš„ `source_platform`, `source_url`, `source` å’Œ `image` å­—æ®µã€‚
- `image` å­—æ®µå¿…é¡»åŸæ ·å¤åˆ¶ï¼Œä¸è¦ä¿®æ”¹é“¾æ¥åœ°å€ã€‚
- `source` å­—æ®µè¡¨ç¤ºæ¥æºå¹³å°ï¼ˆbaidu/tencent/toutiaoï¼‰ï¼Œå¿…é¡»ä¿ç•™ã€‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰
ä½ å¿…é¡»è¾“å‡ºä¸€ä¸ªåŒ…å« "news" å­—æ®µçš„ JSON å¯¹è±¡ã€‚
"news" å­—æ®µæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¿…é¡»ä¸¥æ ¼åŒ…å« 10 æ¡æ•°æ®ï¼ˆRank 0 ä¸ºæ€»ç»“ + Rank 1-9 ä¸º 9 æ¡ç²¾é€‰æ–°é—»ï¼‰ã€‚

{{
  "news": [
    {{ 
      "rank": 0, 
      "title": "çˆ†ç‚¸æ€§ç–‘é—®æ ‡é¢˜ï¼Ÿ", 
      "content": "",
      "source_platform": "",
      "source_url": "",
      "source": "",
      "image": "" 
    }},
    {{ 
      "rank": 1, 
      "title": "æ–°é—»1æ ‡é¢˜", 
      "content": "æ–°é—»1æ­£æ–‡...", 
      "source_platform": "æ¥æºå¹³å°åç§°", 
      "source_url": "åŸå§‹æ–‡ç« é“¾æ¥", 
      "source": "baidu",
      "image": "åŸå§‹å›¾ç‰‡é“¾æ¥(å¿…é¡»ä¿ç•™)" 
    }},
    {{ 
      "rank": 2, 
      "title": "æ–°é—»2æ ‡é¢˜", 
      "content": "æ–°é—»2æ­£æ–‡...", 
      "source_platform": "æ¥æºå¹³å°åç§°", 
      "source_url": "åŸå§‹æ–‡ç« é“¾æ¥", 
      "source": "tencent",
      "image": "åŸå§‹å›¾ç‰‡é“¾æ¥" 
    }},
    ...
    {{ "rank": 9, ... }}
  ]
}}

æ³¨æ„ï¼šRank 0 çš„ title å¿…é¡»ä»¥é—®å·ï¼ˆ?ï¼‰ç»“å°¾ã€‚
æ³¨æ„ï¼šRank 0 çš„ contentã€source_platformã€source_urlã€sourceã€image å­—æ®µå‡ç•™ç©ºã€‚
Rank 1-9 çš„æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»å¡«å……å®Œæ•´ã€‚
"""

def clean_output_dir():
    """æ¸…ç©ºè¾“å‡ºç›®å½•"""
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR)

def load_history():
    """åŠ è½½å†å²åº“"""
    if not os.path.exists(HISTORY_FILE):
        print(f"  [*] å†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·¯å¾„: {HISTORY_FILE}")
        return []
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
            return history if isinstance(history, list) else []
    except Exception as e:
        print(f"  [!] å†å²æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return []

def save_history(news_list):
    """ä¿å­˜å†å²åº“ï¼Œæœ€å¤šä¿ç•™36æ¡"""
    history = load_history()
    
    # æ·»åŠ æ–°é¡¹ç›®ï¼ˆrank 1-9ï¼‰
    for item in news_list:
        if item.get('rank', 0) > 0:  # è·³è¿‡rank 0çš„æ‘˜è¦
            history.append({
                "title": item.get('title', ''),
                "content": item.get('content', ''),
                "source_platform": item.get('source_platform', ''),
                "timestamp": datetime.utcnow().isoformat()
            })
    
    # åªä¿ç•™æœ€æ–°çš„36æ¡ï¼ˆ4*9=36ï¼‰
    initial_count = len(history)
    if len(history) > 36:
        history = history[-36:]
        deleted_count = initial_count - 36
        print(f"  å†å²åº“å·²æ›´æ–°ï¼šä¿ç•™æœ€æ–°36æ¡ï¼Œåˆ é™¤æ—©æœŸ{deleted_count}æ¡")
    else:
        print(f"  å†å²åº“å·²æ›´æ–°ï¼šå½“å‰{len(history)}æ¡")
    
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def save_polished_news(polished_data):
    """ä¿å­˜æ¶¦è‰²åçš„æ–°é—»JSONæ–‡ä»¶"""
    timestamp = polished_data.get('timestamp', '')
    filename = f"{OUTPUT_DIR}/homenews_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(polished_data, f, ensure_ascii=False, indent=2)
    
    print(f"  [âœ“] æ¶¦è‰²æ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")
    return filename

def fetch_news_from_scrapers(count=9):
    """è°ƒç”¨ä¸‰ä¸ªæ–°é—»æŠ“å–è„šæœ¬"""
    print("\n" + "="*50)
    print("ğŸ“° [Scraping] å¼€å§‹ä»å„å¹³å°æŠ“å–æ–°é—»")
    print("="*50)
    all_news = []
    
    try:
        from .fetch_baidu import get_baidu_news
        source1_data = get_baidu_news(count)
        if source1_data:
            all_news.extend(source1_data)
            print(f"ğŸ“Š [Summary] Baidu: âœ“ æˆåŠŸæŠ“å– {len(source1_data)} æ¡")
    except ImportError:
        try:
            from fetch_baidu import get_baidu_news
            source1_data = get_baidu_news(count)
            if source1_data:
                all_news.extend(source1_data)
                print(f"ğŸ“Š [Summary] Baidu: âœ“ æˆåŠŸæŠ“å– {len(source1_data)} æ¡")
        except Exception as e:
            print(f"ğŸ“Š [Summary] Baidu: âœ— å¤±è´¥ - {e}")
    except Exception as e:
        print(f"ğŸ“Š [Summary] Baidu: âœ— å¤±è´¥ - {e}")
    
    try:
        from .fetch_tencent import get_tencent_news
        source2_data = get_tencent_news(count)
        if source2_data:
            all_news.extend(source2_data)
            print(f"ğŸ“Š [Summary] Tencent: âœ“ æˆåŠŸæŠ“å– {len(source2_data)} æ¡")
    except ImportError:
        try:
            from fetch_tencent import get_tencent_news
            source2_data = get_tencent_news(count)
            if source2_data:
                all_news.extend(source2_data)
                print(f"ğŸ“Š [Summary] Tencent: âœ“ æˆåŠŸæŠ“å– {len(source2_data)} æ¡")
        except Exception as e:
            print(f"ğŸ“Š [Summary] Tencent: âœ— å¤±è´¥ - {e}")
    except Exception as e:
        print(f"ğŸ“Š [Summary] Tencent: âœ— å¤±è´¥ - {e}")
    
    try:
        from .fetch_toutiao import get_toutiao_news
        source3_data = get_toutiao_news(count)
        if source3_data:
            all_news.extend(source3_data)
            print(f"ğŸ“Š [Summary] Toutiao: âœ“ æˆåŠŸæŠ“å– {len(source3_data)} æ¡")
    except ImportError:
        try:
            from fetch_toutiao import get_toutiao_news
            source3_data = get_toutiao_news(count)
            if source3_data:
                all_news.extend(source3_data)
                print(f"ğŸ“Š [Summary] Toutiao: âœ“ æˆåŠŸæŠ“å– {len(source3_data)} æ¡")
        except Exception as e:
            print(f"ğŸ“Š [Summary] Toutiao: âœ— å¤±è´¥ - {e}")
    except Exception as e:
        print(f"ğŸ“Š [Summary] Toutiao: âœ— å¤±è´¥ - {e}")
    
    print("="*50)
    print(f"âœ“ å…¨éƒ¨å¹³å°æŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(all_news)} æ¡æ–°é—»å€™é€‰\n")
    return all_news

def call_deepseek_api(all_news_items, history_context, max_retries=3):
    """è°ƒç”¨DeepSeek APIè¿›è¡Œæ¶¦è‰²"""
    print("\n" + "-"*30)
    print("ğŸ¤– [AI] æ­£åœ¨å¯åŠ¨æ–°é—»æ¶¦è‰²ä¸ç­›é€‰...")
    print("-"*30)

    if not DEEPSEEK_API_KEY:
        print("  [!] é”™è¯¯: æœªæ‰¾åˆ° DEEPSEEK_API_KEYã€‚")
        return None

    # Format History
    history_str = "æ— å†å²è®°å½•"
    if history_context:
        history_lines = [f"- {h.get('title')} ({h.get('timestamp', '')[:10]})" for h in history_context]
        history_str = "\n".join(history_lines[:10])  # åªæ˜¾ç¤ºæœ€æ–°10æ¡

    input_payload = []
    for item in all_news_items:
        content_text = item.get('content', '')
        if len(content_text) > 800:
            content_text = content_text[:800] + "..."
            
        entry = {
            "title": item.get('title', ''),
            "content": content_text,
            "source_platform": item.get('source_platform', 'Unknown'),
            "source_url": item.get('source_url', ''),
            "image": item.get('image', '')
        }
        if entry['title'] or entry['content']:
            input_payload.append(entry)

    json_payload_str = json.dumps(input_payload, ensure_ascii=False)
    print(f"  [>] å‘é€ {len(input_payload)} æ¡å€™é€‰æ–°é—» + {len(history_context)} æ¡å†å²è®°å½•ç»™ AI...")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    # æ³¨å…¥å†å²è®°å½•åˆ°æç¤ºè¯
    final_system_prompt = SYSTEM_PROMPT.format(history_context_str=history_str)
    
    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": final_system_prompt},
            {"role": "user", "content": json_payload_str}
        ],
        "temperature": 0.3,
        "response_format": {"type": "json_object"},
        "stream": False
    }

    for attempt in range(max_retries):
        try:
            start_time = time.time()
            response = requests.post(DEEPSEEK_BASE_URL, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            result_json = response.json()
            answer_content = result_json["choices"][0]["message"]["content"]
            parsed_data = json.loads(answer_content)
            
            elapsed = time.time() - start_time
            if isinstance(parsed_data, dict) and "news" in parsed_data:
                news_list = parsed_data["news"]
                if not isinstance(news_list, list):
                    print(f"  [!] Error: 'news' field is not a list: {type(news_list)}")
                    continue
                
                print(f"  [âœ“] AI æ¶¦è‰²å®Œæˆ ({elapsed:.1f}s). ç»“æœæ•°é‡: {len(news_list)}")
                if len(news_list) > 0:
                    summary = news_list[0].get('title', 'No Summary Title')
                    print(f"      æ€»ç»“æ ‡é¢˜: {summary}")
                
                if len(news_list) < 10:
                    print(f"  [!] è­¦å‘Š: AI è¿”å›æ¡ç›®å°‘äºé¢„æœŸ ({len(news_list)}/10)")
                
                return parsed_data
            else:
                print(f"  [!] API response format unexpected.")

        except Exception as e:
            print(f"  [!] AI API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(3)
                
    print("  [!] æ‰€æœ‰é‡è¯•å‡å¤±è´¥ã€‚")
    return None

def main(count=9):
    """ä¸»æµç¨‹"""
    print("\n" + "="*30)
    print("ğŸš€ [Home News] å¼€å§‹æ¶¦è‰²æµç¨‹")
    print("="*30)
    
    # æ¸…ç©ºè¾“å‡ºç›®å½•
    print("  æ­£åœ¨æ¸…ç†è¾“å‡ºç›®å½•...")
    clean_output_dir()
    
    # åŠ è½½å†å²åº“
    history_items = load_history()
    print(f"  åŠ è½½å†å²åº“ï¼š{len(history_items)} æ¡")
    
    # æŠ“å–æ–°é—»
    all_news = fetch_news_from_scrapers(count)
    
    if not all_news:
        print("  [!] æœªèƒ½æŠ“å–ä»»ä½•æ–°é—»")
        return None
    
    # è°ƒç”¨DeepSeek APIè¿›è¡Œæ¶¦è‰²
    polished_data = call_deepseek_api(all_news, history_items)
    
    if not polished_data:
        print("  [!] AIæ¶¦è‰²å¤±è´¥")
        return None
    
    # æ·»åŠ æ—¶é—´æˆ³
    beijing_time = datetime.utcnow() + timedelta(hours=8)
    timestamp = beijing_time.strftime("%Y%m%d_%H%M%S")
    
    # ç¡®ä¿æ–°é—»åˆ—è¡¨å­˜åœ¨
    news_list = polished_data.get('news', [])
    
    # é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„ï¼šåªä¿ç•™å¿…è¦å­—æ®µï¼Œsource_platform ä½¿ç”¨æ–°é—»æºè€Œä¸æ˜¯ author
    for item in news_list:
        if item.get('rank', 0) > 0:  # è·³è¿‡ rank 0
            # source_platform å·²ç»åœ¨çˆ¬è™«ä¸­è®¾ç½®ä¸ºæ–°é—»æºå¹³å°ï¼Œè¿™é‡Œä¿æŒä¸å˜
            pass
    
    # ä¿å­˜ä¸­é—´æ–‡ä»¶åˆ° output ç›®å½•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    polished_data_with_timestamp = {
        "news": news_list,
        "timestamp": timestamp
    }
    save_polished_news(polished_data_with_timestamp)
    
    print(f"  [âœ“] æ–°é—»æ¶¦è‰²å®Œæˆï¼Œå…± {len(news_list)} æ¡ã€‚")
    
    # æ›´æ–°å†å²åº“
    print("  æ­£åœ¨æ›´æ–°å†å²åº“...")
    save_history(news_list)
    
    # è¿”å›ç»™ pipeline çš„æ•°æ®ï¼ˆä¸åŒ…å« timestampï¼‰
    return {"news": news_list}

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='æ–°é—»æ¶¦è‰²ä¸èšåˆå™¨')
    parser.add_argument('--count', type=int, default=9, help='æ¯ä¸ªå¹³å°æŠ“å–æ•°é‡ï¼ˆé»˜è®¤9ï¼‰')
    args = parser.parse_args()

    data = main(args.count)
    if data:
        print(json.dumps(data, ensure_ascii=False, indent=2))
