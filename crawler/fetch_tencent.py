import requests
from bs4 import BeautifulSoup
import json
import time
import random
import os
import sys
import re

# é…ç½®
TAG_CONFIG = {
    "morning": {
        "id": "aEWqxLtdgmQ=",
        "name": "æ—©æŠ¥",
        "json_prefix": "tencent_morning_news",
        "img_dir": "images/morning"
    },
    "evening": {
        "id": "bEeox7NdhmM=",
        "name": "æ™šæŠ¥",
        "json_prefix": "tencent_evening_news",
        "img_dir": "images/evening"
    }
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://news.qq.com/",
}

def install_selenium_hint():
    print("\n" + "!"*50)
    print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° Selenium åº“ã€‚")
    print("ä¸ºäº†å®ç°å…¨è‡ªåŠ¨åŒ–æŠ“å–ï¼ˆç»•è¿‡è…¾è®¯çš„åŠ¨æ€åŠ å¯†åˆ—è¡¨ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ€æœ¯ã€‚")
    print("è¯·åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š")
    print("\n    pip install selenium webdriver-manager\n")
    print("å®‰è£…å®Œæˆåï¼Œè¯·é‡æ–°è¿è¡Œæœ¬è„šæœ¬ã€‚")
    print("!"*50 + "\n")
    sys.exit(1)

def get_links_auto(tag_id, count):
    """ä½¿ç”¨ Selenium è‡ªåŠ¨æ§åˆ¶æµè§ˆå™¨è·å–åŠ¨æ€åˆ—è¡¨"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.chrome.service import Service
        from webdriver_manager.chrome import ChromeDriverManager
    except ImportError:
        install_selenium_hint()

    url = f"https://news.qq.com/tag/{tag_id}"
    print(f"æ­£åœ¨å¯åŠ¨åå°æµè§ˆå™¨ï¼Œè®¿é—®ï¼š{url}")
    print(f"ç›®æ ‡ï¼šæŠ“å–å‰ {count} æ¡é“¾æ¥...")

    links = []
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(f"user-agent={HEADERS['User-Agent']}")

    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get(url)
        time.sleep(3) # ç­‰å¾…åˆå§‹åŠ è½½
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        retry_count = 0
        
        while len(links) < count and retry_count < 3:
            elements = driver.find_elements(By.CSS_SELECTOR, "a")
            current_found = 0
            for el in elements:
                try:
                    href = el.get_attribute("href")
                    # è…¾è®¯æ–°é—»æ–‡ç« é€šå¸¸åŒ…å« /rain/a/ æˆ– /omn/
                    # ä¸¥æ ¼è¿‡æ»¤æ‰ author(ä½œè€…é¡µ), video(è§†é¢‘é¡µ), zt(ä¸“é¢˜é¡µ)
                    if href and ("/rain/a/" in href or "/omn/" in href) \
                       and "author" not in href \
                       and "video" not in href \
                       and "zt" not in href \
                       and href not in links:
                        links.append(href)
                        current_found += 1
                        print(f"[{len(links)}] å‘ç°: {href}")
                        if len(links) >= count:
                            break
                except:
                    continue
            
            if len(links) >= count:
                break
                
            # æ»šåŠ¨åŠ è½½
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                retry_count += 1
                print("é¡µé¢æœªåˆ·æ–°ï¼Œå°è¯•ç»§ç»­ç­‰å¾…...")
            else:
                retry_count = 0
            last_height = new_height

    except Exception as e:
        print(f"æµè§ˆå™¨è‡ªåŠ¨åŒ–å‡ºé”™: {e}")
    finally:
        if driver:
            driver.quit()
            
    return links[:count]

def download_image(url, folder, index):
    if not url: return "æ— å›¾ç‰‡"
    if not os.path.exists(folder):
        os.makedirs(folder)
        
    try:
        # Filter out common logo/icon URLs strings
        if "icon" in url or "logo" in url:
            return None

        response = requests.get(url, headers={"User-Agent": HEADERS["User-Agent"]}, stream=True, timeout=10)
        if response.status_code == 200:
            content_type = response.headers.get('Content-Type', '').lower()
            ext = '.webp' if 'webp' in content_type else '.png' if 'png' in content_type else '.jpg'
            filename = f"{folder}/{index}{ext}"
            
            # Write to file
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(1024): f.write(chunk)
            
            # Check file size (Filter out small icons < 10KB)
            file_size = os.path.getsize(filename)
            if file_size < 10240: # 10KB
                os.remove(filename)
                return None # Signal to try next image
                
            return filename
    except: pass
    return "ä¸‹è½½å¤±è´¥"

def get_article_details(url, index, config):
    print(f"[{index}] è§£æ: {url}")
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Check if it's a video URL (Contains 'V' in the ID, e.g., 2025...V...)
        # Regex to check for the V pattern in the last segment of URL
        is_video = False
        if re.search(r'/[a-zA-Z0-9]*V[a-zA-Z0-9]*', url):
            is_video = True

        # --- 1. Title Extraction ---
        title = ""
        # Strategy A: Standard Tags
        title_tag = soup.find('h1') or \
                    soup.find('div', class_='video-title') or \
                    soup.find('h2', class_='title')
        
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # Strategy B: <title> Tag (Fallback & Video Preference)
        # If title is missing OR it's a video (often <title> is cleaner than div.video-title), use <title>
        if not title or is_video:
            if soup.title:
                full_title = soup.title.get_text().strip()
                # Clean suffixes like "_è…¾è®¯æ–°é—»", "_è…¾è®¯ç½‘"
                title = full_title.split('_')[0].strip()

        if not title: title = "æœªæ‰¾åˆ°æ ‡é¢˜"

        # --- 2. Content Extraction ---
        content = ""
        # Standard Article Content
        content_div = soup.find('div', class_='content-article') or soup.find('div', id='ArticleContent')
        
        if content_div:
            content = content_div.get_text(strip=True)
        else:
            # Video Page Logic
            if is_video:
                # Try finding description first
                desc = soup.find('div', class_='video-desc') or \
                       soup.find('p', class_='desc') or \
                       soup.find('meta', attrs={'name': 'description'})
                
                if desc:
                    if hasattr(desc, 'get_text'):
                        content = desc.get_text(strip=True)
                    elif 'content' in desc.attrs:
                        content = desc['content']
                
                # USER REQUIREMENT: For videos, if no detailed content, use Title
                if not content or len(content) < 5:
                    content = title
            else:
                # Text Page Fallback
                paragraphs = soup.find_all('p')
                valid_ps = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10]
                content = "\n".join(valid_ps)

        if not content: content = title # Ultimate fallback

        # --- 3. Source Platform ---
        source_platform = "æœªçŸ¥å¹³å°"
        
        # Priority 1: Specific OMN/Rain author selectors
        author_info = soup.select_one(".author-info .name") or \
                      soup.select_one(".media-info .media-name") or \
                      soup.select_one(".author-name") or \
                      soup.select_one(".media-name")
        
        if author_info:
            source_platform = author_info.get_text(strip=True)
        
        # Priority 2: Meta tags (be careful with generic ones)
        if not source_platform or source_platform in ["æœªçŸ¥å¹³å°", "è…¾è®¯ç½‘", "è…¾è®¯æ–°é—»"]:
            author_meta = soup.find('meta', property='article:author') or \
                          soup.find('meta', attrs={'name': 'author'})
            if author_meta:
                source_platform = author_meta.get('content', source_platform)
        
        # Priority 3: Other common selectors
        if not source_platform or source_platform in ["æœªçŸ¥å¹³å°", "è…¾è®¯ç½‘", "è…¾è®¯æ–°é—»"]:
            media_elem = soup.find('div', class_='author-txt') or \
                         soup.find('div', class_='author-name') or \
                         soup.find('span', class_='media-name')
            if media_elem:
                source_platform = media_elem.get_text(strip=True)

        # Final check: If still "è…¾è®¯ç½‘", check og:site_name but only as last resort
        if not source_platform or source_platform == "æœªçŸ¥å¹³å°":
            meta_site = soup.find('meta', property='og:site_name')
            if meta_site:
                source_platform = meta_site.get('content', 'æœªçŸ¥å¹³å°')

        # --- 4. Image Extraction ---
        cover_image_url = ""
        # Strategy A: OG Image
        og_img = soup.find('meta', property='og:image')
        if og_img and og_img.get('content'):
            cover_image_url = og_img['content']
        
        # Strategy B: First image in content
        if not cover_image_url or "default" in cover_image_url:
            if content_div:
                img = content_div.find('img')
                if img:
                    cover_image_url = img.get('data-src') or img.get('src')
        
        # Strategy C: Video Poster (if video)
        if not cover_image_url and is_video:
            video_tag = soup.find('video')
            if video_tag:
                cover_image_url = video_tag.get('poster')

        local_image_path = ""
        if cover_image_url and cover_image_url.startswith('http'):
            local_image_path = cover_image_url # Return REMOTE URL for pipeline

        return {
            "åºå·": index,
            "æ ‡é¢˜": title,
            "å†…å®¹": content,
            "æºå¹³å°": source_platform,
            "æºå¹³å°çš„é“¾æ¥": url,
            "å°é¢å›¾ç‰‡": local_image_path
        }
    except Exception as e:
        print(f"è§£æå¤±è´¥: {e}")
        return None

def main(report_type="morning", limit=10, out_dir=None):
    print("\n" + "-"*30)
    print(f"ğŸ” [Tencent] Starting Hot News Scraper ({report_type})")
    print("-"*30)
    
    if out_dir is None:
        out_dir = os.getenv('TENCENT_OUT_DIR', 'tencent')
    
    # 1. é€‰æ‹©ç±»å‹ (report_type maps to news_type logic)
    news_type = report_type
        
    if news_type not in TAG_CONFIG:
        print(f"  [!] Unknown report type: {news_type}, defaulting to morning")
        news_type = "morning"

    cfg = TAG_CONFIG[news_type]
    print(f"  [âœ“] Target: ã€{cfg['name']}ã€‘")
    
    # 2. è®¾ç½®æ•°é‡
    target_count = limit
        
    # 3. å…¨è‡ªåŠ¨è·å–é“¾æ¥
    links = get_links_auto(cfg['id'], target_count)
    
    if not links:
        print("  [!] No links found from Tencent tag page.")
        return []

    print(f"  [âœ“] Successfully discovered {len(links)} links. Parsing content...")
    
    all_data = []
    for i, link in enumerate(links, 1):
        print(f"\n  [#{i}] Processing: {link[:70]}...")
        data = get_article_details(link, i, cfg)
        if data:
            print(f"      - Title: {data['æ ‡é¢˜']}")
            print(f"      - Source: {data['æºå¹³å°']}")
            if data['å°é¢å›¾ç‰‡']:
                print(f"      - Image URL found")

            # Map Chinese keys to English keys for pipeline compatibility
            record = {
                "rank": data["åºå·"],
                "title": data["æ ‡é¢˜"],
                "content": data["å†…å®¹"],
                "source_platform": data["æºå¹³å°"], 
                "source_url": data["æºå¹³å°çš„é“¾æ¥"],
                "image": data["å°é¢å›¾ç‰‡"] # REMOTE URL
            }
            all_data.append(record)
        time.sleep(random.uniform(0.5, 1.0))
        
    # 4. ä¿å­˜ç»“æœ (Optional standalone output)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    filename = os.path.join(out_dir, f"{cfg['json_prefix']}_{len(all_data)}pcs.json")
    with open(filename, "w", encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)
        
    print(f"\n  [âœ“] Tencent scraping complete. Total items: {len(all_data)}")
    return all_data

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Tencent News Scraper")
    parser.add_argument("--limit", type=int, default=10, help="Number of news items to fetch")
    parser.add_argument("--out-dir", type=str, default=None, help="Output directory")
    parser.add_argument("--type", type=str, default="morning", choices=["morning", "evening"], help="Report type")
    args = parser.parse_args()
    
    main(limit=args.limit, out_dir=args.out_dir, report_type=args.type)
