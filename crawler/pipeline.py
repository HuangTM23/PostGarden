import os
import json
import sys
import shutil
import zipfile
from datetime import datetime, timedelta

# Add crawler subdirectories to path
sys.path.append(os.path.dirname(__file__))

# Import modules
from homenews import home_polish
from worldnews import world_polish
from entertainment import ent_polish
import image_utils

# --- Configuration ---
OUTPUT_DIR = "output"
LATEST_VERSION_FILE = os.path.join(OUTPUT_DIR, "latest_versions.json")

def ensure_dirs():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def update_latest_version(section, zip_filename):
    """Updates the latest_versions.json file."""
    versions = {}
    if os.path.exists(LATEST_VERSION_FILE):
        try:
            with open(LATEST_VERSION_FILE, 'r') as f:
                versions = json.load(f)
        except:
            pass
    
    versions[section] = zip_filename
    
    with open(LATEST_VERSION_FILE, 'w') as f:
        json.dump(versions, f, indent=4)
    print(f"  [âœ“] æ›´æ–°æœ€æ–°ç‰ˆæœ¬è®°å½•: {section} -> {zip_filename}")

def package_section(section_prefix, polished_data, timestamp_str):
    """
    é€šç”¨æ‰“åŒ…å‡½æ•°ï¼š
    1. ä¸‹è½½/å¤„ç†å›¾ç‰‡ (é•¿å›¾ç”¨å…¬ç‰ˆæ›¿ä»£)
    2. å¤±è´¥åˆ™ä½¿ç”¨å…¬ç‰ˆå›¾ç‰‡
    3. ç”Ÿæˆ ZIP åŒ…
    """
    print(f"\n[{section_prefix}] æ­£åœ¨æ‰“åŒ…æ•°æ®...")
    
    # ä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(OUTPUT_DIR, f"temp_{section_prefix}_{timestamp_str}")
    temp_images_dir = os.path.join(temp_dir, "images")
    os.makedirs(temp_images_dir, exist_ok=True)
    
    try:
        polished_items = polished_data.get("news", [])
        
        # å…ˆä¿å­˜è°ƒè¯•ç‰ˆï¼ˆä¿å­˜åŸå§‹çš„ polished_dataï¼Œå›¾ç‰‡URLæœªä¿®æ”¹ï¼‰
        debug_json_path = os.path.join(OUTPUT_DIR, f"test_{section_prefix}_{timestamp_str}.json")
        with open(debug_json_path, 'w', encoding='utf-8') as f:
            json.dump(polished_data, f, ensure_ascii=False, indent=4)
        print(f"  [âœ“] è°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: test_{section_prefix}_{timestamp_str}.json")
        
        # 1. å¤„ç†å›¾ç‰‡
        print(f"  æ­£åœ¨å¤„ç† {len(polished_items)-1} æ¡æ–°é—»å›¾ç‰‡...")
        
        for item in polished_items:
            rank = item.get("rank", 0)
            if rank == 0: 
                continue

            remote_url = item.get("image", "")
            title = item.get('title', 'NoTitle')
            author = item.get('author', '')
            
            # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            raw_prefix = title[:6]
            safe_prefix = "".join([c if c.isalnum() or c in ('-', '_') else '_' for c in raw_prefix])
            if not safe_prefix: 
                safe_prefix = "Img"
            
            # ç»Ÿä¸€ä½¿ç”¨ jpg æˆ–æ ¹æ®åŸ url åç¼€
            ext = ".jpg"
            if remote_url and ".png" in remote_url.lower(): 
                ext = ".png"
            if remote_url and ".webp" in remote_url.lower(): 
                ext = ".webp"
            
            filename = f"rank{rank}_{safe_prefix}_{timestamp_str}{ext}"
            local_path = os.path.join(temp_images_dir, filename)
            rel_path = f"images/{filename}"
            
            success = False
            
            # å°è¯•ä¸‹è½½å¹¶å¤„ç†ï¼ˆåŒ…å«é•¿å›¾æ£€æµ‹ï¼‰
            if remote_url and remote_url.startswith("http"):
                success = image_utils.download_and_process(remote_url, local_path)
            
            # å¤±è´¥æˆ–æ— æ•ˆ URLï¼Œä½¿ç”¨å…¬ç‰ˆå›¾ç‰‡
            if not success:
                print(f"    [!] å›¾ç‰‡è·å–å¤±è´¥ (Rank {rank})ï¼Œä½¿ç”¨å…¬ç‰ˆå›¾ç‰‡: {author}")
                success = image_utils.copy_placeholder(author, local_path)

            # æ›´æ–° item.image å­—æ®µ
            if success:
                item["image"] = rel_path
            else:
                # å½»åº•å¤±è´¥ï¼Œä¿ç•™ç©ºå€¼æˆ–ä½¿ç”¨é»˜è®¤å€¼
                item["image"] = ""
        
        # 2. ç»Ÿä¸€åŒ– JSON æ ¼å¼ï¼šåªä¿ç•™å¿…è¦å­—æ®µ
        # å¯¹äº ZIP åŒ…å†…çš„ JSONï¼Œåªä¿ç•™ï¼šrank, title, source_platform, source_url, content, image
        cleaned_news = []
        for item in polished_items:
            cleaned_item = {
                "rank": item.get("rank", 0),
                "title": item.get("title", ""),
                "original_title": item.get("title0", ""),
                "source_platform": item.get("source_platform", ""),
                "source_url": item.get("source_url", ""),
                "content": item.get("content", ""),
                "image": item.get("image", "")
            }
            cleaned_news.append(cleaned_item)
            
        # 3. ä¿å­˜ç”¨äº zip çš„ jsonï¼ˆå›¾ç‰‡è·¯å¾„å·²ä¿®æ”¹ä¸ºæœ¬åœ°ç›¸å¯¹è·¯å¾„ï¼‰
        json_filename_in_zip = f"polished_all_{timestamp_str}.json"
        json_path_temp = os.path.join(temp_dir, json_filename_in_zip)
        
        # é‡æ–°ç»„ç»‡ JSON é¡ºåºï¼šnews æ•°ç»„ + timestamp
        polished_data_ordered = {
            "news": cleaned_news,
            "timestamp": timestamp_str
        }
        
        with open(json_path_temp, 'w', encoding='utf-8') as f:
            json.dump(polished_data_ordered, f, ensure_ascii=False, indent=4)
            
        # 4. åˆ›å»º ZIP
        zip_name = f"{section_prefix}_{timestamp_str}.zip"
        zip_path = os.path.join(OUTPUT_DIR, zip_name)
        
        print(f"  æ­£åœ¨ç”Ÿæˆå‹ç¼©åŒ…: {zip_name}")
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            zf.write(json_path_temp, arcname=json_filename_in_zip)
            for root, _, files in os.walk(temp_images_dir):
                for file in files:
                    abs_path = os.path.join(root, file)
                    rel_path = f"images/{file}"
                    zf.write(abs_path, arcname=rel_path)
                        
        print(f"  [âœ“] æ‰“åŒ…å®Œæˆã€‚")
        update_latest_version(section_prefix.lower(), zip_name)
        
        return True

    finally:
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

# --- HOME NEWS PIPELINE ---
def run_home_news(count=9):
    print("\n" + "="*40)
    print("ğŸ  [Home] å¼€å§‹æ‰§è¡Œå›½å†…æ–°é—»æµç¨‹")
    print("="*40)
    
    try:
        # è°ƒç”¨ä¸»æµç¨‹ï¼Œä¼ å…¥æ–°é—»æ•°é‡å‚æ•°
        polished = home_polish.main(count=count)
        
        if not polished or "news" not in polished:
            print("  [!] å›½å†…æ–°é—»æ¶¦è‰²å¤±è´¥ã€‚")
            return None
        
        return polished
        
    except Exception as e:
        print(f"  [!] Home æµç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None

# --- WORLD NEWS PIPELINE ---
def run_world_news(count=9):
    print("\n" + "="*40)
    print("ğŸŒ [World] å¼€å§‹æ‰§è¡Œå›½é™…æ–°é—»æµç¨‹")
    print("="*40)
    
    try:
        # è°ƒç”¨ä¸»æµç¨‹ï¼Œä¼ å…¥æ–°é—»æ•°é‡å‚æ•°ï¼ˆä½¿ç”¨ limit å‚æ•°åï¼‰
        world_polish.main(limit=count)
        
        # ä» worldnews/output è¯»å–æœ€æ–°ç”Ÿæˆçš„æ–‡ä»¶
        worldnews_output = os.path.join(os.path.dirname(__file__), "worldnews", "output")
        if os.path.exists(worldnews_output):
            files = os.listdir(worldnews_output)
            json_files = [f for f in files if f.endswith('.json')]
            if json_files:
                json_files.sort(reverse=True)
                latest_json = json_files[0]
                json_path = os.path.join(worldnews_output, latest_json)
                
                with open(json_path, 'r', encoding='utf-8') as f:
                    polished_data = json.load(f)
                
                polished_data = {"news": polished_data} if isinstance(polished_data, list) else polished_data
                return polished_data
            else:
                print("  [!] æœªæ‰¾åˆ°ç”Ÿæˆçš„ JSON æ–‡ä»¶ã€‚")
                return None
        else:
            print("  [!] worldnews/output ç›®å½•ä¸å­˜åœ¨ã€‚")
            return None
        
    except Exception as e:
        print(f"  [!] World æµç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None

# --- ENTERTAINMENT NEWS PIPELINE ---
def run_entertainment_news(count=9):
    print("\n" + "="*40)
    print("ğŸ‰ [Entertainment] å¼€å§‹æ‰§è¡Œå¨±ä¹æ–°é—»æµç¨‹")
    print("="*40)
    
    try:
        # è°ƒç”¨ä¸»æµç¨‹ï¼Œä¼ å…¥æ–°é—»æ•°é‡å‚æ•°
        polished_data = ent_polish.aggregate_news(count=count)
        
        if not polished_data or not polished_data.get("news"):
            print("  [!] å¨±ä¹æ–°é—»èšåˆå¤±è´¥ã€‚")
            return None
        
        return polished_data
        
    except Exception as e:
        print(f"  [!] Entertainment æµç¨‹å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return None

def cleanup_output_directory():
    """
    æ¸…ç† output ç›®å½•ï¼š
    1. åˆ é™¤å…¶ä»–é ZIPã€é latest_versions.jsonã€é test_*.json çš„æ–‡ä»¶
    2. å¯¹äºæ¯ä¸ªå¹³å°ï¼ˆHome/World/Entertainmentï¼‰ï¼Œåªä¿ç•™æœ€æ–°çš„ 1 ä¸ª ZIP
    3. ä¿ç•™æœ€æ–°çš„ 3 ä¸ª test_*.json è°ƒè¯•æ–‡ä»¶ï¼ˆæ¯ä¸ªå¹³å° 1 ä¸ªï¼‰
    4. æœ€ç»ˆç»“æœï¼š3 ä¸ªæœ€æ–° ZIP + 3 ä¸ª test JSON + latest_versions.json
    """
    print("\n" + "="*40)
    print("ğŸ§¹ [Cleanup] æ¸…ç†è¾“å‡ºç›®å½•")
    print("="*40)
    
    if not os.path.exists(OUTPUT_DIR):
        print("  output ç›®å½•ä¸å­˜åœ¨")
        return
    
    all_files = os.listdir(OUTPUT_DIR)
    
    # 1. åˆ é™¤å…¶ä»–é ZIPã€é latest_versions.jsonã€é test_*.json çš„æ–‡ä»¶
    print("  æ­£åœ¨åˆ é™¤æ— æ•ˆæ–‡ä»¶...")
    invalid_files = [
        f for f in all_files 
        if not f.endswith('.zip') 
        and f != 'latest_versions.json' 
        and not f.startswith('test_')
        and not f.startswith('temp_')
        and not f.endswith('_history.json') # é˜²æ­¢è¯¯åˆ å†å²è®°å½•æ–‡ä»¶
    ]
    for f in invalid_files:
        try:
            full_path = os.path.join(OUTPUT_DIR, f)
            if os.path.isfile(full_path):
                os.remove(full_path)
                print(f"    [âœ“] åˆ é™¤: {f}")
        except Exception as e:
            print(f"    [!] åˆ é™¤å¤±è´¥ {f}: {e}")
    
    # 2. æ¸…ç†è¿‡æœŸçš„ ZIP åŒ…ï¼ˆæ¯ä¸ªå¹³å°åªä¿ç•™æœ€æ–°çš„ 1 ä¸ªï¼‰
    print("  æ­£åœ¨æ¸…ç†è¿‡æœŸ ZIP åŒ…...")
    prefixes = ["Home_", "World_", "Entertainment_"]
    
    for prefix in prefixes:
        zip_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(prefix) and f.endswith(".zip")]
        
        if not zip_files:
            print(f"    {prefix}: æœªæ‰¾åˆ° ZIP æ–‡ä»¶")
            continue
        
        # æŒ‰æ—¶é—´æˆ³å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        zip_files.sort(reverse=True)
        
        print(f"    {prefix}: ç°æœ‰ {len(zip_files)} ä¸ªï¼Œä¿ç•™æœ€æ–° 1 ä¸ª")
        
        # åˆ é™¤é™¤äº†æœ€æ–°çš„ä»¥å¤–çš„æ‰€æœ‰æ–‡ä»¶
        for zip_file in zip_files[1:]:
            try:
                full_path = os.path.join(OUTPUT_DIR, zip_file)
                os.remove(full_path)
                print(f"      [âœ“] åˆ é™¤æ—§ç‰ˆæœ¬: {zip_file}")
            except Exception as e:
                print(f"      [!] åˆ é™¤å¤±è´¥ {zip_file}: {e}")
    
    # 3. æ¸…ç†è¿‡æœŸçš„ test_*.json è°ƒè¯•æ–‡ä»¶ï¼ˆæ¯ä¸ªå¹³å°åªä¿ç•™æœ€æ–°çš„ 1 ä¸ªï¼‰
    print("  æ­£åœ¨æ¸…ç†è¿‡æœŸè°ƒè¯•æ–‡ä»¶...")
    test_prefixes = ["test_Home_", "test_World_", "test_Entertainment_"]
    
    for test_prefix in test_prefixes:
        test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(test_prefix) and f.endswith(".json")]
        
        if not test_files:
            continue
        
        # æŒ‰æ—¶é—´æˆ³å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        test_files.sort(reverse=True)
        
        # åˆ é™¤é™¤äº†æœ€æ–°çš„ä»¥å¤–çš„æ‰€æœ‰è°ƒè¯•æ–‡ä»¶
        for test_file in test_files[1:]:
            try:
                full_path = os.path.join(OUTPUT_DIR, test_file)
                os.remove(full_path)
                print(f"    [âœ“] åˆ é™¤æ—§è°ƒè¯•æ–‡ä»¶: {test_file}")
            except Exception as e:
                print(f"    [!] åˆ é™¤å¤±è´¥ {test_file}: {e}")
    
    # 4. éªŒè¯æœ€ç»ˆçŠ¶æ€
    print("\n  æœ€ç»ˆæ–‡ä»¶çŠ¶æ€ï¼š")
    remaining_files = os.listdir(OUTPUT_DIR)
    zip_count = 0
    test_count = 0
    
    for f in sorted(remaining_files):
        file_path = os.path.join(OUTPUT_DIR, f)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            if size > 1024*1024:
                size_str = f"{size / (1024*1024):.1f} MB"
            elif size > 1024:
                size_str = f"{size / 1024:.1f} KB"
            else:
                size_str = f"{size} B"
            print(f"    âœ“ {f} ({size_str})")
            
            if f.endswith('.zip'):
                zip_count += 1
            elif f.startswith('test_') and f.endswith('.json'):
                test_count += 1
    
    print(f"\n  [âœ“] æ¸…ç†å®Œæˆã€‚ä¿ç•™ {zip_count} ä¸ª ZIP + {test_count} ä¸ªè°ƒè¯•æ–‡ä»¶ + latest_versions.json")

def cleanup_intermediate_dirs():
    """æ¸…ç†ä¸´æ—¶ç›®å½•"""
    print("\n  æ­£åœ¨æ¸…ç†ä¸´æ—¶æŠ“å–ç›®å½•...")
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dirs_to_remove = [
        "bbc_news_data", "cnn_data", "nytimes_data", "sky_news_data", 
        "SampleNewsG", "RawData_Backup"
    ]
    for d in dirs_to_remove:
        path = d if os.path.isabs(d) else os.path.join(base_dir, d)
        if os.path.exists(path):
            try: 
                shutil.rmtree(path)
            except Exception: 
                pass

def main():
    print("\n" + "#"*50)
    print(f"ğŸš€ å¯åŠ¨ PostGarden å…¨æµç¨‹çˆ¬è™«ä»»åŠ¡")
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("#"*50)

    ensure_dirs()
    
    # Timestamp for packaging
    beijing_time = datetime.utcnow() + timedelta(hours=8)
    timestamp = beijing_time.strftime('%Y%m%d_%H%M%S')
    print(f"â³ å…¨å±€æ—¶é—´æˆ³ (åŒ—äº¬æ—¶é—´): {timestamp}")
    
    # é…ç½®ï¼šæ¯ä¸ªå¹³å°æŠ“å–çš„æ–°é—»æ•°é‡
    news_count = 9
    
    # è¿è¡Œä¸‰å¤§æ¿å—ï¼Œåªä¼ å…¥æ–°é—»æ•°é‡å‚æ•°
    print("\n" + "="*50)
    print("ğŸ“‹ å¯åŠ¨å„å¹³å°æ•°æ®é‡‡é›†å’Œæ¶¦è‰²")
    print("="*50)
    
    home_data = run_home_news(count=news_count)
    world_data = run_world_news(count=news_count)
    ent_data = run_entertainment_news(count=news_count)
    
    # æ‰“åŒ…é˜¶æ®µï¼šä½¿ç”¨æ—¶é—´æˆ³ä¸º ZIP å‘½å
    print("\n" + "="*50)
    print("ğŸ“¦ å¯åŠ¨æ•°æ®æ‰“åŒ…é˜¶æ®µ")
    print("="*50)
    
    if home_data:
        package_section("Home", home_data, timestamp)
    
    if world_data:
        package_section("World", world_data, timestamp)
    
    if ent_data:
        package_section("Entertainment", ent_data, timestamp)
    
    # æ”¶å°¾
    cleanup_output_directory()
    cleanup_intermediate_dirs()
    
    print("\n" + "#"*50)
    print("âœ… å…¨æµç¨‹ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")
    print("#"*50 + "\n")

if __name__ == "__main__":
    main()