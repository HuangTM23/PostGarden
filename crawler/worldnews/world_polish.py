"""
World News Polish Script
å›½é™…æ–°é—»æ¶¦è‰²ä¸èšåˆä¸»ç¨‹åº
"""
import os
import json
import requests
import argparse
import shutil
from datetime import datetime
from dotenv import load_dotenv

# Import scrapers
try:
    from . import fetch_bbc
    from . import fetch_cnn
    from . import fetch_nytimes
    from . import fetch_sky
except ImportError:
    import fetch_bbc
    import fetch_cnn
    import fetch_nytimes
    import fetch_sky

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# --- Configuration ---
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
API_URL = "https://api.deepseek.com/chat/completions"
MODEL_NAME = "deepseek-chat"

HISTORY_FILE = os.path.join(os.path.dirname(__file__), "worldnews_history.json")
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
MAX_HISTORY_SIZE = 36  # å†å²åº“æœ€å¤§å®¹é‡

# V2 Prompt Template
V2_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸­æ–‡å›½é™…æ–°é—»ç¼–è¾‘ï¼Œè´Ÿè´£åˆ¶ä½œä¸€æœŸå›½é™…æ–°é—»ç²¾é€‰å†…å®¹ã€‚

æ ¸å¿ƒç›®æ ‡ï¼šä»"æœ€æ–°æŠ“å–æ–°é—»"ä¸­ï¼Œé€šè¿‡"äº‹ä»¶çº§å»é‡ + å†…å®¹ç­›é€‰"ï¼Œç›´æ¥é€‰å‡º 9 æ¡"å®Œå…¨ä¸åŒæ–°é—»äº‹ä»¶"çš„å›½é™…æ–°é—»ã€‚

ã€é‡è¦ï¼šå†å²æ’é‡å‚è€ƒã€‘
ä»¥ä¸‹æ˜¯è¿‡å»å·²å‘å¸ƒè¿‡çš„æ–°é—»ï¼ˆHistoryï¼‰ï¼Œè¯·ä¸¥æ ¼å›é¿ä¸è¿™äº›å†å²å†…å®¹é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„äº‹ä»¶ï¼š
{history_context_str}

âš ï¸ å¼ºåˆ¶è¿‡æ»¤è§„åˆ™ (Negative Filter) - ä¼˜å…ˆçº§æœ€é«˜
å¿…é¡»å‰”é™¤ä»¥ä¸‹æ–°é—»ï¼š
1. æ¶‰åŠä¸­å›½å›½å†…çš„æ”¿æ²»ã€æ³•å¾‹ã€æ”¿åºœå†³ç­–ç­‰ã€‚
2. æ¶‰åŠä¸­å›½å†›äº‹ã€å›½é˜²ã€é¢†åœŸäº‰è®®ç­‰ã€‚
3. é‡å¤çš„å›½é™…äº‹ä»¶ã€‚

ä¿ç•™çš„æ–°é—»åº”ä¾§é‡äºï¼šå…¨çƒç§‘æŠ€ä¸å•†ä¸šã€é‡å¤§å›½é™…åœ°ç¼˜æ”¿æ²»ï¼ˆéä¸­å›½ç›¸å…³ï¼‰ã€æ°‘ç”Ÿä¸ç¤¾ä¼šçƒ­ç‚¹ã€æ–‡åŒ–ã€ä½“è‚²ã€å¥‡é—»ã€‚

å†™ä½œè¦æ±‚ï¼š
- ä½¿ç”¨ä¸“ä¸šã€æ­£å¼çš„æ–°é—»ä½“ã€‚
- æ¯æ¡æ–°é—»æ­£æ–‡ï¼šä¸è¶…è¿‡ 50 ä¸ªæ±‰å­—ï¼Œåªä¿ç•™"å‘ç”Ÿäº†ä»€ä¹ˆ + å…³é”®ç»“æœ"ã€‚
- å•æ¡æ–°é—»æ ‡é¢˜ï¼šä¸è¶…è¿‡ 20 ä¸ªæ±‰å­—ã€‚
- **å¿…é¡»ç¿»è¯‘**ï¼šå°†è‹±æ–‡æ ‡é¢˜å’Œå†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

**Rank 0 æ€»ç»“æ ‡é¢˜ï¼ˆæ ¸å¿ƒä»»åŠ¡ï¼‰**ï¼š
ä½ æ˜¯ä¸€åé¡¶çº§"æ ‡é¢˜å…š"ç¼–è¾‘ï¼Œæ“…é•¿åˆ¶ä½œ**æå…·çˆ†ç‚¸æ€§å’Œå†²å‡»åŠ›çš„æ–°é—»æ ‡é¢˜**ã€‚

ã€æ ¸å¿ƒç­–ç•¥ã€‘
1. **ä¸å¿…é¢é¢ä¿±åˆ°**ï¼šå¯ä»¥åªèšç„¦ 9 æ¡æ–°é—»ä¸­**æœ€å…·å†²å‡»åŠ›çš„ 1-2 æ¡äº‹ä»¶**
2. **åˆ¶é€ ç´§è¿«æ„Ÿ**ï¼šä½¿ç”¨"æ­£åœ¨"ã€"çªå‘"ã€"ç´§æ€¥"ç­‰è¯æ±‡
3. **çªå‡ºå¯¹æŠ—**ï¼šå¼ºè°ƒå†²çªã€åšå¼ˆã€æ’•è£‚ã€åè½¬
4. **å¼•å‘å¥½å¥‡**ï¼šå¯ä»¥ç”¨ç–‘é—®å¥æˆ–æ„Ÿå¹å¥ç»“å°¾
5. **å…·ä½“èƒœäºæŠ½è±¡**ï¼šå¯ä»¥æåŠå…·ä½“å›½å®¶/åœ°åŒº/äº‹ä»¶ï¼Œä½†è¦æœ‰å†²å‡»åŠ›

ã€æ ‡é¢˜å…¬å¼ï¼ˆä»»é€‰å…¶ä¸€ï¼‰ã€‘
- **å¯¹æŠ—å‹**ï¼šXX vs XXï¼è°å°†èƒœå‡ºï¼Ÿ
- **å±æœºå‹**ï¼šXXå‘Šæ€¥ï¼XXé¢ä¸´å´©æºƒè¾¹ç¼˜
- **åè½¬å‹**ï¼šæƒŠå¤©åè½¬ï¼XXçªç„¶â€¦â€¦
- **ç–‘é—®å‹**ï¼šXXä¸ºä½•çªç„¶â€¦â€¦ï¼ŸçœŸç›¸ä»¤äººéœ‡æƒŠ
- **çˆ†æ–™å‹**ï¼šç‹¬å®¶ï¼XXå†…å¹•æ›å…‰
- **è¶‹åŠ¿å‹**ï¼šXXå¤±æ§ï¼å…¨çƒXXé™·å…¥æ··ä¹±

ã€æ ‡é¢˜è¦æ±‚ã€‘
- å­—æ•°ï¼š**15-25 ä¸ªæ±‰å­—**
- é£æ ¼ï¼š**çˆ†ç‚¸æ€§ã€åˆºæ¿€æ€§ã€å†²çªå¯¹æŠ—**
- **å¿…é¡»ä½¿ç”¨**ï¼š"ï¼"æˆ–"ï¼Ÿ"ç»“å°¾
- **å¯ä»¥åŒ…å«**ï¼šå…·ä½“å›½å®¶ã€åœ°åŒºã€äººç‰©ã€äº‹ä»¶åç§°
- **ç¦æ­¢**ï¼šå±è¨€è€¸å¬ã€è™šå‡ä¿¡æ¯ã€è¿‡åº¦å¤¸å¼ 

ã€ç¤ºä¾‹å‚è€ƒã€‘
- "æ ¼é™µå…°ä¸»æƒäº‰å¤ºæˆ˜ï¼ç¾ä¸¹å…³ç³»é™·å…¥ç©ºå‰å±æœº"
- "AIå·¨å¤´çªç„¶å´©ç›˜ï¼Ÿç¡…è°·éœ‡è¡ä¸æ­¢"
- "æ¬§æ´²èƒ½æºå‘Šæ€¥ï¼å†¬å­£æ–­æ°”å±æœºè¿«åœ¨çœ‰ç«"
- "ç‰¹æœ—æ™®å›å½’å€’è®¡æ—¶ï¼å…¨çƒç§©åºé¢ä¸´é‡ç»„ï¼Ÿ"
- "ä¸­ä¸œå’Œå¹³åè®®ç ´è£‚ï¼æˆ˜ç«é‡ç‡ƒåœ¨å³"

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆ JSON æ ¼å¼ï¼‰
ä½ å¿…é¡»è¾“å‡ºä¸€ä¸ªåŒ…å« 10 æ¡æ•°æ®çš„åˆ—è¡¨ï¼ˆRank 0 ä¸ºæ€»ç»“ + Rank 1-9 ä¸º 9 æ¡ç²¾é€‰æ–°é—»ï¼‰ã€‚

æ³¨æ„ï¼šRank 0 éœ€è¦åŒ…å«æ‰€æœ‰å­—æ®µï¼Œä½† contentã€title0ã€source_platformã€source_urlã€indexã€authorã€image éƒ½ç•™ç©ºã€‚

[
  {{
    "rank": 0,
    "title": "çˆ†ç‚¸æ€§ä¸­æ–‡æ€»ç»“æ ‡é¢˜ï¼æˆ–ï¼Ÿ",
    "title0": "",
    "content": "",
    "index": 0,
    "author": "",
    "source_platform": "",
    "source_url": "",
    "image": ""
  }},
  {{
    "rank": 1,
    "title": "ä¸­æ–‡æ–°é—»æ ‡é¢˜",
    "title0": "åŸå§‹è‹±æ–‡æ ‡é¢˜",
    "source_platform": "æ¥æºå¹³å°",
    "source_url": "åŸå§‹é“¾æ¥",
    "content": "50å­—ä»¥å†…ä¸­æ–‡æ­£æ–‡",
    "index": çƒ­åº¦æŒ‡æ•°,
    "author": "å¹³å°åç§°",
    "image": "å›¾ç‰‡URL"
  }},
  ... (ç›´åˆ° Rank 9)
]

ä»¥ä¸‹æ˜¯åŸå§‹æ–°é—»æ•°æ®:
{news_data}
"""

def clear_output_directory():
    """æ¸…ç©ºè¾“å‡ºç›®å½•"""
    if os.path.exists(OUTPUT_DIR):
        try:
            shutil.rmtree(OUTPUT_DIR)
            print(f"[âœ“] å·²æ¸…ç©ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        except Exception as e:
            print(f"[!] æ¸…ç©ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def setup_directories():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def load_history():
    """åŠ è½½å†å²æ–°é—»è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
                return []
        except:
            return []
    return []

def save_history(news_items):
    """ä¿å­˜æ–°é—»åˆ°å†å²è®°å½•ï¼Œç»´æŠ¤æœ€å¤§å®¹é‡36æ¡"""
    try:
        history = load_history()
        
        # æ·»åŠ æ–°æ–°é—»åˆ°å†å²åº“
        for item in news_items:
            if item.get('rank', 0) > 0:  # è·³è¿‡ rank 0
                history.append({
                    'title': item.get('title'),
                    'title0': item.get('title0', ''),
                    'date': datetime.now().strftime('%Y-%m-%d')
                })
        
        # åªä¿ç•™æœ€è¿‘ 36 æ¡
        history = history[-MAX_HISTORY_SIZE:]
        
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
            
        print(f"[âœ“] å†å²åº“å·²æ›´æ–°ï¼Œå½“å‰åŒ…å« {len(history)} æ¡è®°å½•")
    except Exception as e:
        print(f"[!] ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

def print_news_item(platform, index, total, item):
    """æ‰“å°å•æ¡æ–°é—»ä¿¡æ¯"""
    print(f"\n  [{index}/{total}] {item.get('title', 'N/A')[:70]}")
    print(f"       æ¥æº: {item.get('source_platform', 'N/A')}")
    # æ‰“å°å®Œæ•´ URL
    print(f"       é“¾æ¥: {item.get('source_url', 'N/A')}")
    if item.get('image'):
        # æ‰“å°å®Œæ•´å›¾ç‰‡ URL
        print(f"       å›¾ç‰‡: {item.get('image')}")

def run_scrapers(limit=10):
    """è¿è¡Œæ‰€æœ‰æŠ“å–è„šæœ¬å¹¶è¿”å›æ•°æ®"""
    print("\n" + "="*50)
    print("ğŸ” å¯åŠ¨å›½é™…æ–°é—»æŠ“å–")
    print("="*50)
    
    all_news = []
    
    # 1. BBC
    print("\n[1/4] æŠ“å– BBC News...")
    try:
        bbc_data = fetch_bbc.scrape(limit)
        if bbc_data:
            for idx, item in enumerate(bbc_data, 1):
                print_news_item("BBC", idx, len(bbc_data), item)
            all_news.extend(bbc_data)
            print(f"\n  âœ“ BBC å®Œæˆ: {len(bbc_data)} æ¡")
        else:
            print(f"  âœ— BBC: æœªè·å–åˆ°æ•°æ®")
    except Exception as e:
        print(f"  âœ— BBC å¤±è´¥: {e}")
    
    # 2. CNN
    print("\n[2/4] æŠ“å– CNN...")
    try:
        cnn_data = fetch_cnn.scrape(limit)
        if cnn_data:
            for idx, item in enumerate(cnn_data, 1):
                print_news_item("CNN", idx, len(cnn_data), item)
            all_news.extend(cnn_data)
            print(f"\n  âœ“ CNN å®Œæˆ: {len(cnn_data)} æ¡")
        else:
            print(f"  âœ— CNN: æœªè·å–åˆ°æ•°æ®")
    except Exception as e:
        print(f"  âœ— CNN å¤±è´¥: {e}")
    
    # 3. NYTimes
    print("\n[3/4] æŠ“å– NYTimes...")
    try:
        nyt_data = fetch_nytimes.scrape(limit)
        if nyt_data:
            for idx, item in enumerate(nyt_data, 1):
                print_news_item("NYTimes", idx, len(nyt_data), item)
            all_news.extend(nyt_data)
            print(f"\n  âœ“ NYTimes å®Œæˆ: {len(nyt_data)} æ¡")
        else:
            print(f"  âœ— NYTimes: æœªè·å–åˆ°æ•°æ®")
    except Exception as e:
        print(f"  âœ— NYTimes å¤±è´¥: {e}")
    
    # 4. Sky News
    print("\n[4/4] æŠ“å– Sky News...")
    try:
        sky_data = fetch_sky.scrape(limit)
        if sky_data:
            for idx, item in enumerate(sky_data, 1):
                print_news_item("Sky News", idx, len(sky_data), item)
            all_news.extend(sky_data)
            print(f"\n  âœ“ Sky News å®Œæˆ: {len(sky_data)} æ¡")
        else:
            print(f"  âœ— Sky News: æœªè·å–åˆ°æ•°æ®")
    except Exception as e:
        print(f"  âœ— Sky News å¤±è´¥: {e}")
    
    print(f"\n{'='*50}")
    print(f"æ±‡æ€»: å…±è·å– {len(all_news)} æ¡åŸå§‹æ–°é—»")
    print("="*50)
    return all_news

def call_deepseek(all_news, history_context=[]):
    """è°ƒç”¨ DeepSeek API è¿›è¡Œç­›é€‰ã€ç¿»è¯‘å’Œæ¶¦è‰²"""
    print("\n" + "="*50)
    print("ğŸ¤– è°ƒç”¨ DeepSeek AI è¿›è¡Œå†…å®¹å¤„ç†")
    print("="*50)
    
    # Format History
    history_str = "æ— å†å²è®°å½•"
    if history_context:
        history_lines = [
            f"- {h.get('title')} / {h.get('title0', '')} ({h.get('date')})" 
            for h in history_context
        ]
        history_str = "\n".join(history_lines)
    
    print(f"è¾“å…¥: {len(all_news)} æ¡å€™é€‰æ–°é—»")
    print(f"å†å²: {len(history_context)} æ¡è®°å½•")
    
    # æ„é€  Prompt
    news_json_str = json.dumps(all_news, ensure_ascii=False, indent=2)
    prompt = V2_PROMPT_TEMPLATE.format(
        news_data=news_json_str,
        history_context_str=history_str
    )
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,
    }

    try:
        print("æ­£åœ¨è¯·æ±‚ DeepSeek API...")
        response = requests.post(API_URL, headers=headers, json=data, timeout=120)
        response.raise_for_status()
        result = response.json()
        content_str = result['choices'][0]['message']['content']
        
        # Cleanup markdown
        content_str = content_str.replace("```json", "").replace("```", "").strip()
        
        # Simple JSON repair
        if content_str.startswith("{") and "}{" in content_str:
            content_str = f"[{content_str.replace('}{', '},{')}]"
        elif not content_str.startswith("["):
            start = content_str.find("[")
            end = content_str.rfind("]")
            if start != -1 and end != -1:
                content_str = content_str[start:end+1]
        
        final_data = json.loads(content_str)
        
        if isinstance(final_data, dict):
            if "news" in final_data:
                final_data = final_data["news"]
            else:
                final_data = [final_data]
        
        # ç¡®ä¿ Rank 0 åŒ…å«æ‰€æœ‰å­—æ®µ
        if final_data and final_data[0].get('rank') == 0:
            rank0 = final_data[0]
            rank0.setdefault('title0', '')
            rank0.setdefault('content', '')
            rank0.setdefault('index', 0)
            rank0.setdefault('author', '')
            rank0.setdefault('source_platform', '')
            rank0.setdefault('source_url', '')
            rank0.setdefault('image', '')
        
        print(f"[âœ“] DeepSeek è¿”å› {len(final_data)} æ¡ç»“æœ")
        return final_data
        
    except Exception as e:
        print(f"[!] DeepSeek API é”™è¯¯: {e}")
        return None

def main(limit=9):
    """ä¸»å‡½æ•°"""
    # 0. æ¸…ç©ºè¾“å‡ºç›®å½•
    clear_output_directory()
    
    # 1. åŠ è½½å†å²åº“
    history = load_history()
    print(f"\nå†å²åº“: {len(history)} æ¡è®°å½•")
    
    # 2. æŠ“å–æ–°é—»
    raw_news = run_scrapers(limit=limit)
    
    if not raw_news:
        print("\n[!] æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œé€€å‡ºã€‚")
        return None
    
    # 3. è°ƒç”¨ DeepSeek å¤„ç†
    final_news = call_deepseek(raw_news, history_context=history)
    
    if not final_news:
        print("\n[!] AI å¤„ç†å¤±è´¥ï¼Œé€€å‡ºã€‚")
        return None
    
    # 4. ä¿å­˜è¾“å‡ºæ–‡ä»¶åˆ° worldnews/output ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = os.path.join(OUTPUT_DIR, f"worldnews_{timestamp}.json")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_news, f, ensure_ascii=False, indent=2)
        print(f"\n[âœ“] å·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"\n[!] ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    # 5. æ›´æ–°å†å²åº“
    save_history(final_news)
    
    # 6. è¾“å‡ºç»“æœæ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“° å¤„ç†å®Œæˆ")
    print("="*50)
    print(f"æ€»ç»“æ ‡é¢˜: {final_news[0].get('title', 'N/A')}")
    print(f"ç²¾é€‰æ–°é—»: {len(final_news) - 1} æ¡")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print("="*50 + "\n")
    
    # è¿”å›æ•°æ®ç»™ pipelineï¼ˆè¿”å›åˆ—è¡¨æ ¼å¼ï¼‰
    return final_news

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="World News Scraper and Polish Tool")
    parser.add_argument('--limit', type=int, default=10, 
                       help="æ¯ä¸ªå¹³å°æŠ“å–çš„æ–°é—»æ•°é‡ (é»˜è®¤: 10)")
    args = parser.parse_args()
    
    main(limit=args.limit)