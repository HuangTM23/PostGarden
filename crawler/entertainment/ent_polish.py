#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆå¨±ä¹æ–°é—»èšåˆå™¨
åŒ…å«å¹³å°æŠ“å–ã€AIç­›é€‰å’Œå†…å®¹èšåˆåŠŸèƒ½
"""

import json
import sys
import argparse
import time
import os
import requests
import shutil
from datetime import datetime, timedelta

# Import the three scraper modules
try:
    from .get_tencent_entertainment_hot import get_tencent_entertainment_hot
    from .get_douyin_rank import get_douyin_rank
    from .get_bilibili_rank import get_bilibili_rank
except ImportError:
    from get_tencent_entertainment_hot import get_tencent_entertainment_hot
    from get_douyin_rank import get_douyin_rank
    from get_bilibili_rank import get_bilibili_rank

# é…ç½®
HISTORY_FILE = os.path.join(os.path.dirname(__file__), "entertainment_history.json")
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"

def clean_output_dir():
    """æ¸…ç©ºè¾“å‡ºç›®å½•"""
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
        print(f"  å·²æ¸…ç©ºè¾“å‡ºç›®å½•ï¼š{OUTPUT_DIR}")
    os.makedirs(OUTPUT_DIR)

def load_history():
    """åŠ è½½å†å²åº“"""
    if not os.path.exists(HISTORY_FILE):
        return []
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
            return history if isinstance(history, list) else []
    except:
        return []

def save_history(new_items):
    """ä¿å­˜å†å²åº“ï¼Œæœ€å¤šä¿ç•™36æ¡"""
    history = load_history()
    
    # æ·»åŠ æ–°é¡¹ç›®ï¼ˆæ¯æ¬¡æ·»åŠ 9æ¡ï¼‰
    for item in new_items:
        history.append({
            "title": item.get('title', ''),
            "content": item.get('content', ''),
            "source_platform": item.get('source_platform', ''),
            "timestamp": datetime.utcnow().isoformat()
        })
    
    # åªä¿ç•™æœ€æ–°çš„36æ¡ï¼ˆ4*9=36ï¼‰
    initial_count = len(history)
    if len(history) > 36:
        history = history[-36:]
        deleted_count = initial_count - 36
        print(f"  å†å²åº“å·²æ›´æ–°ï¼šä¿ç•™æœ€æ–°36æ¡ï¼Œåˆ é™¤æ—©æœŸ{deleted_count}æ¡")
    else:
        print(f"  å†å²åº“å·²æ›´æ–°ï¼šå½“å‰{len(history)}æ¡")
    
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def save_aggregated_news(polished_data):
    """ä¿å­˜èšåˆçš„æ–°é—»JSONæ–‡ä»¶"""
    timestamp = polished_data.get('timestamp', '')
    filename = f"{OUTPUT_DIR}/entertainment_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(polished_data, f, ensure_ascii=False, indent=2)
    
    print(f"  [âœ“] èšåˆæ•°æ®å·²ä¿å­˜è‡³ï¼š{filename}")
    return filename

def generate_clickbait_title(selected_news):
    """ç”Ÿæˆåˆºæ¿€æ€§ã€çˆ†ç‚¸æ€§çš„æ‘˜è¦æ ‡é¢˜"""
    if not selected_news:
        beijing_time = datetime.utcnow() + timedelta(hours=8)
        return f"å¨±ä¹èµ„è®¯ç²¾é€‰ | {beijing_time.strftime('%mæœˆ%dæ—¥')}çƒ­ç‚¹"
    
    # æå–å…³é”®è¯å’Œæ˜æ˜Ÿåå­—
    keywords = []
    for item in selected_news[:5]:
        title = item.get('title', '')
        # ç®€å•çš„å…³é”®è¯æå–
        if 'æ‹çˆ±' in title or 'å©šç¤¼' in title or 'å®˜å®£' in title:
            keywords.append('æ„Ÿæƒ…å…«å¦')
        if 'åˆ†æ‰‹' in title or 'ç¦»å©š' in title or 'å‡ºè½¨' in title:
            keywords.append('å©šå˜é£æ³¢')
        if 'äº‰è®®' in title or 'è¢«éª‚' in title or 'ç¿»è½¦' in title:
            keywords.append('æ˜æ˜Ÿé£æ³¢')
        if 'æ–°å‰§' in title or 'å¼€æ’­' in title or 'ä¸Šæ˜ ' in title:
            keywords.append('ä½œå“çƒ­è®®')
        if 'éŸ³ä¹' in title or 'æ¼”å”±ä¼š' in title:
            keywords.append('éŸ³ä¹ç››äº‹')
    
    beijing_time = datetime.utcnow() + timedelta(hours=8)
    date_str = beijing_time.strftime('%mæœˆ%dæ—¥')
    
    # åˆºæ¿€æ€§æ ‡é¢˜æ¨¡æ¿åº“
    titles = [
        f"ã€ç‚¸è£‚ã€‘{date_str}å¨±ä¹åœˆåˆé—¹ç¿»å¤©ï¼è¿™äº›å¤§ç“œä½ ç»å¯¹æƒ³ä¸åˆ°",
        f"ã€éœ‡æ’¼ã€‘æ˜æ˜Ÿä»¬çš„ç§˜å¯†åœ¨è¿™é‡Œï¼{date_str}æœ€ç«çˆ†å†…å¹•å¤§æ­éœ²",
        f"ã€çˆ†æ–™ã€‘{date_str}å¨±ä¹åœˆé»‘å¹•é‡é‡ï¼Œè¿™äº›äº‹å„¿ä½ å¯èƒ½æ°¸è¿œéƒ½ä¸çŸ¥é“",
        f"ã€ç‹¬å®¶ã€‘{date_str}æœ€çƒ­è®®è¯é¢˜å…¨åœ¨è¿™ï¼ç½‘å‹åµç¿»å¤©çš„å‰§æƒ…",
        f"ã€åè½¬ã€‘{date_str}ä¸€å¤©å†…å‘ç”Ÿçš„å¨±ä¹åœˆå¤§äº‹ï¼Œè®©äººæªæ‰‹ä¸åŠ",
        f"ã€å¿…çœ‹ã€‘è¿™ä¸ª{date_str}ï¼Œå¨±ä¹åœˆçš„ç“œå¤§åˆ°åœä¸ä¸‹æ¥ï¼",
        f"ã€äº‰è®®ã€‘{date_str}æœ€å…·è¯é¢˜æ€§æ–°é—»ï¼Œæœ‰äººæ¬¢å–œæœ‰äººå¿§",
        f"ã€æ›å…‰ã€‘{date_str}å¨±ä¹åœˆéšç’çš„æ•…äº‹ï¼Œä»Šå¤©å…¨éƒ¨æµ®å‡ºæ°´é¢",
        f"ã€çƒ­è®®ã€‘{date_str}ç½‘å‹ç–¯ç‹‚è®¨è®ºçš„è¯é¢˜ï¼Œç©¶ç«Ÿæ˜¯ä»€ä¹ˆçœŸç›¸ï¼Ÿ",
        f"ã€è½°åŠ¨ã€‘{date_str}å¨±ä¹åœˆå†èµ·é£æ³¢ï¼Œä¼—æ˜æ˜Ÿæˆ–è¢«å·å…¥å…¶ä¸­",
    ]
    
    # æ ¹æ®å…³é”®è¯é€‰æ‹©æœ€åˆé€‚çš„æ ‡é¢˜
    if 'å©šå˜é£æ³¢' in keywords:
        titles = [
            f"ã€å©šå˜ã€‘{date_str}å¨±ä¹åœˆæ„Ÿæƒ…çº¿åˆå´©äº†ï¼å¤šä½æ˜æ˜Ÿå·å…¥æ„Ÿæƒ…é£æ³¢",
            f"ã€çˆ†ç‚¸ã€‘{date_str}ä¸€è¿ä¸²åˆ†æ‰‹å®˜å®£éœ‡æ’¼å¨±ä¹åœˆï¼Œç²‰ä¸é›†ä½“å´©æºƒ",
            f"ã€åè½¬ã€‘æ„Ÿæƒ…æ•…äº‹åå¤æ— å¸¸ï¼Œ{date_str}æ˜æ˜Ÿçˆ±æƒ…çº¿è°œå›¢é‡é‡",
        ]
    elif 'æ˜æ˜Ÿé£æ³¢' in keywords:
        titles = [
            f"ã€ç¿»è½¦ã€‘{date_str}å¤šä½æ˜æ˜Ÿé™·äº‰è®®ï¼Œç½‘å‹è®¨ä¼å£°ä¸æ–­",
            f"ã€é£æ³¢ã€‘{date_str}æ˜æ˜Ÿè¨€è¡Œä¸å½“å¼•ä¼—æ€’ï¼Œèˆ†è®ºä¸€è¾¹å€’",
            f"ã€çƒ­è®®ã€‘{date_str}æ˜æ˜Ÿä»¬çš„è¿‡å¾€è¢«æ‰’å‡ºæ¥ï¼Œç½‘å‹åµå¾—ä¸å¯å¼€äº¤",
        ]
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªæ ‡é¢˜ï¼ˆä½¿ç”¨å†…å®¹é•¿åº¦ä½œä¸ºä¼ªéšæœºï¼‰
    title_index = len(selected_news) % len(titles)
    return titles[title_index]

def deduplicate_with_deepseek(all_news, history_items):
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œæ™ºèƒ½å»é‡"""
    if not DEEPSEEK_API_KEY:
        print("  [!] æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œä½¿ç”¨æœ¬åœ°å»é‡")
        return deduplicate_locally(all_news, history_items)
    
    print("  æ­£åœ¨ä½¿ç”¨DeepSeekè¿›è¡Œæ™ºèƒ½å»é‡...")
    
    # å‡†å¤‡æç¤ºè¯
    history_titles = [item['title'] for item in history_items]
    all_titles = [f"{i+1}. {item['title']}" for i, item in enumerate(all_news)]
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå†…å®¹å»é‡ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æœ€æ–°çš„å¨±ä¹æ–°é—»åˆ—è¡¨ä¸­é€‰æ‹©9ä¸ªä¸å†å²è®°å½•æ— å…³ä¸”äº’ç›¸ä¸é‡å¤çš„æ–°é—»ã€‚

ã€å†å²è®°å½•ã€‘ï¼ˆéœ€è¦é¿å…é‡å¤çš„å†…å®¹ï¼‰ï¼š
{chr(10).join(history_titles)}

ã€æœ€æ–°æ–°é—»åˆ—è¡¨ã€‘ï¼š
{chr(10).join(all_titles)}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š
1. é€‰å‡º9æ¡ä¸å†å²è®°å½•å®Œå…¨ä¸åŒçš„æ–°é—»
2. è¿™9æ¡æ–°é—»ä¹‹é—´ä¹Ÿåº”è¯¥äº’ä¸é‡å¤
3. ä¼˜å…ˆé€‰æ‹©ä¸åŒæ¥æºçš„æ–°é—»
4. è¿”å›ç»“æœæ ¼å¼ä¸ºJSONï¼ŒåŒ…å«selected_indiceså­—æ®µï¼Œå€¼ä¸ºé€‰ä¸­çš„æ–°é—»ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰ï¼Œä¾‹å¦‚ï¼š{{"selected_indices": [0, 2, 5, 7, 9, 10, 12, 15, 18]}}

åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
"""
    
    try:
        response = requests.post(
            DEEPSEEK_API_URL,
            headers={
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 500
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # è§£æJSONå“åº”
            selected_data = json.loads(content)
            indices = selected_data.get('selected_indices', [])
            
            # è¿‡æ»¤é€‰ä¸­çš„æ–°é—»
            selected_news = [all_news[i] for i in indices if i < len(all_news)]
            selected_news = selected_news[:9]
            
            print(f"  [âœ“] DeepSeekå»é‡å®Œæˆï¼Œé€‰å‡º {len(selected_news)} æ¡æ–°é—»")
            return selected_news
        else:
            print(f"  [!] DeepSeek APIè¿”å›é”™è¯¯: {response.status_code}")
            return deduplicate_locally(all_news, history_items)
            
    except Exception as e:
        print(f"  [!] DeepSeekè°ƒç”¨å¤±è´¥: {e}")
        return deduplicate_locally(all_news, history_items)

def deduplicate_locally(all_news, history_items):
    """æœ¬åœ°æ™ºèƒ½å»é‡"""
    print("  æ­£åœ¨è¿›è¡Œæœ¬åœ°å»é‡...")
    
    history_titles = set(item['title'] for item in history_items)
    titles_set = set()
    selected_items = []

    def is_political_or_military(title):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¿æ²»æˆ–å†›äº‹ç›¸å…³å†…å®¹"""
        political_keywords = ['æ”¿æ²»', 'æ”¿åºœ', 'å®˜å‘˜', 'æ”¿ç­–', 'å†›', 'å†›é˜Ÿ', 'å†›äº‹', 'æˆ˜äº‰', 'å¤–äº¤', 'é€‰ä¸¾', 'å…š', 'çºªå§”', 'ç›‘å¯Ÿ', 'äººå¤§', 'æ”¿å', 'å›½å®¶', 'é¢†å¯¼', 'ä¸»å¸­', 'æ€»ç†', 'æ”¿æ²»å±€', 'ä¸­å¤®', 'æ³•é™¢', 'æ£€å¯Ÿé™¢', 'å…¬å®‰', 'è­¦å¯Ÿ', 'æ­¦è­¦', 'éƒ¨é˜Ÿ', 'å›½é˜²', 'å¯¼å¼¹', 'æ ¸æ­¦å™¨', 'è”åˆå›½', 'æŠ•ç¥¨', 'æ”¿å…š', 'è®®ä¼š', 'å›½ä¼š', 'ç«‹æ³•', 'å¸æ³•', 'è¡Œæ”¿', 'å…¬åŠ¡å‘˜', 'å›½ä¼', 'å¤®ä¼', 'å›½èµ„å§”', 'å‘æ”¹å§”', 'è´¢æ”¿éƒ¨', 'å¤®è¡Œ', 'è´§å¸æ”¿ç­–', 'è´¢æ”¿æ”¿ç­–', 'ç»æµæ”¿ç­–', 'è´¸æ˜“æˆ˜', 'åˆ¶è£', 'åœ°ç¼˜', 'å†²çª', 'åŠ¨ä¹±', 'æš´ä¹±', 'æŠ—è®®', 'ç¤ºå¨', 'æ¸¸è¡Œ', 'ç½¢å·¥', 'ç½¢è¯¾', 'ç½¢å¸‚', 'å¼¹åŠ¾', 'é—®è´£', 'è°ƒæŸ¥', 'å®¡æŸ¥', 'å®¡è®¡', 'ç›‘ç£', 'ä¸¾æŠ¥', 'æ§å‘Š', 'èµ·è¯‰', 'å®¡åˆ¤', 'åˆ¤å†³', 'æ‹˜ç•™', 'é€®æ•', 'å®¡è®¯']
        return any(keyword in title for keyword in political_keywords)

    # ä¼˜å…ˆä¿è¯æ¯ä¸ªå¹³å°çš„ä»£è¡¨
    tencent_news = [item for item in all_news if item.get('source_platform') == 'è…¾è®¯å¨±ä¹']
    douyin_news = [item for item in all_news if item.get('source_platform') == 'æŠ–éŸ³çƒ­æ¦œ']
    bilibili_news = [item for item in all_news if item.get('source_platform') == 'Bilibili' or item.get('source_platform') == 'å“”å“©å“”å“©']

    def add_if_valid(source_list, max_count):
        added = 0
        for item in source_list:
            if added >= max_count:
                break
            title = item.get('title', '')
            
            if not title or len(title) < 2:
                continue
            if is_political_or_military(title):
                continue
            if title in history_titles:  # è·Ÿå†å²åº“å»é‡
                continue
            if title in titles_set:  # è·Ÿå½“å‰é€‰æ‹©å»é‡
                continue
            
            titles_set.add(title)
            selected_items.append(item)
            added += 1

    add_if_valid(tencent_news, 3)
    add_if_valid(douyin_news, 3)
    add_if_valid(bilibili_news, 3)
    
    # è¡¥å……ä¸è¶³çš„éƒ¨åˆ†
    all_others = [i for i in all_news if i.get('title') not in titles_set and i.get('title') not in history_titles]
    add_if_valid(all_others, 9 - len(selected_items))

    selected_items = selected_items[:9]
    return selected_items

def aggregate_news(count=9):
    """èšåˆä¸‰ä¸ªå¹³å°çš„å¨±ä¹æ–°é—»"""
    print("\n" + "="*30)
    print("ğŸš€ [Entertainment] å¼€å§‹èšåˆæµç¨‹")
    print("="*30)
    
    # æ¸…ç©ºè¾“å‡ºç›®å½•
    print("  æ­£åœ¨æ¸…ç†è¾“å‡ºç›®å½•...")
    clean_output_dir()
    
    # åŠ è½½å†å²åº“
    history_items = load_history()
    print(f"  åŠ è½½å†å²åº“ï¼š{len(history_items)} æ¡")
    
    # è°ƒç”¨ä¸‰ä¸ªè„šæœ¬è·å–æ•°æ®
    tencent_data = get_tencent_entertainment_hot(count)
    douyin_data = get_douyin_rank(count)
    bilibili_data = get_bilibili_rank(count)

    all_news = []
    
    # å¤„ç†è…¾è®¯æ•°æ®
    if tencent_data:
        for item in tencent_data:
            all_news.append({
                'rank': item.get('rank', 0),
                'title': item.get('title', ''),
                'title0': item.get('title0', ''),
                'content': item.get('content', ''),
                'index': item.get('index', 0),
                'author': item.get('author', ''),
                'source_platform': item.get('source_platform', ''),
                'source_url': item.get('source_url', ''),
                'image': item.get('image', '')
            })

    # å¤„ç†æŠ–éŸ³æ•°æ®
    if douyin_data:
        for item in douyin_data:
            all_news.append({
                'rank': item.get('rank', 0),
                'title': item.get('title', ''),
                'title0': item.get('title0', ''),
                'content': item.get('content', ''),
                'index': item.get('index', 0),
                'author': item.get('author', ''),
                'source_platform': item.get('source_platform', ''),
                'source_url': item.get('source_url', ''),
                'image': item.get('image', '')
            })

    # å¤„ç†å“”å“©å“”å“©æ•°æ®
    if bilibili_data:
        for item in bilibili_data:
            all_news.append({
                'rank': item.get('rank', 0),
                'title': item.get('title', ''),
                'title0': item.get('title0', ''),
                'content': item.get('content', ''),
                'index': item.get('index', 0),
                'author': item.get('author', ''),
                'source_platform': item.get('source_platform', ''),
                'source_url': item.get('source_url', ''),
                'image': item.get('image', '')
            })

    print(f"  æŠ“å–å®Œæˆï¼šå…± {len(all_news)} æ¡æ–°é—»")

    # ä½¿ç”¨DeepSeekæˆ–æœ¬åœ°å»é‡
    selected_news = deduplicate_with_deepseek(all_news, history_items)

    # ç”Ÿæˆåˆºæ¿€æ€§æ‘˜è¦æ ‡é¢˜
    summary_title = generate_clickbait_title(selected_news)

    # æ„å»ºæœ€ç»ˆç»“æœï¼š1ä¸ªæ‘˜è¦ + 9æ¡æ–°é—»
    final_result = []
    
    # æ·»åŠ æ‘˜è¦ï¼ˆrank=0ï¼Œæ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä½†é™¤titleå¤–éƒ½ä¸ºç©ºï¼‰
    summary_item = {
        "rank": 0,
        "title": summary_title,
        "title0": "",
        "content": "",
        "index": 0,
        "author": "",
        "source_platform": "",
        "source_url": "",
        "image": ""
    }
    final_result.append(summary_item)

    # æ·»åŠ 9æ¡æ–°é—»ï¼ˆrankä»1-9ï¼‰
    for i, news_item in enumerate(selected_news, 1):
        processed_item = {
            "rank": i,
            "title": news_item.get('title', ''),
            "title0": news_item.get('title0', ''),
            "content": news_item.get('content', ''),
            "index": news_item.get('index', 0),
            "author": news_item.get('author', ''),
            "source_platform": news_item.get('source_platform', ''),
            "source_url": news_item.get('source_url', ''),
            "image": news_item.get('image', '')
        }
        final_result.append(processed_item)

    beijing_time = datetime.utcnow() + timedelta(hours=8)
    polished_data = {
        "news": final_result,
        "timestamp": beijing_time.strftime("%Y%m%d_%H%M%S"),
        "total": len(final_result)
    }
    
    print(f"  [âœ“] å¨±ä¹æ–°é—»èšåˆå®Œæˆï¼Œå…± {len(final_result)} æ¡ï¼ˆ1ä¸ªæ‘˜è¦+9æ¡æ–°é—»ï¼‰ã€‚")
    
    # ä¿å­˜èšåˆçš„æ–°é—»JSONæ–‡ä»¶
    save_aggregated_news(polished_data)
    
    # æ›´æ–°å†å²åº“
    print("  æ­£åœ¨æ›´æ–°å†å²åº“...")
    save_history(selected_news)
    
    return polished_data

def main():
    parser = argparse.ArgumentParser(description='ç»¼åˆå¨±ä¹æ–°é—»èšåˆå™¨')
    parser.add_argument('--count', type=int, default=9, help='æ¯ä¸ªå¹³å°æŠ“å–æ•°é‡ï¼ˆé»˜è®¤9ï¼‰')
    args = parser.parse_args()

    data = aggregate_news(args.count)
    print(json.dumps(data, ensure_ascii=False, indent=2))

if __name__ == '__main__':
    main()